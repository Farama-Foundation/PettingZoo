<!doctype html>
<html class="no-js" lang="en" data-content_root="../../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark">
    <meta name="description" content="">
    <meta property="og:title" content="PettingZoo Documentation" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="" />
    <meta property="og:url" content="https://pettingzoo.farama.org/tutorials/agilerl/DQN.html" /><meta name="twitter:card" content="summary_large_image"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../../genindex/" /><link rel="search" title="Search" href="../../../search/" /><link rel="next" title="AgileRL: Implementing MADDPG" href="../MADDPG/" /><link rel="prev" title="AgileRL Tutorial" href="../" />
        <link rel="canonical" href="https://pettingzoo.farama.org/tutorials/agilerl/DQN.html" />

    <link rel="shortcut icon" href="../../../_static/favicon.png"/><!-- Generated with Sphinx 7.4.7 and Furo 2023.08.19.dev1 -->
        <title>AgileRL: Implementing DQN - Curriculum Learning and Self-play - PettingZoo Documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?v=3e7f4c72" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?v=49cbaffd" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    <header class="farama-header" aria-label="Farama header">
      <div class="farama-header__container">
        <div class="farama-header__left--mobile">
          <label class="nav-overlay-icon" for="__navigation">
            <div class="visually-hidden">Toggle site navigation sidebar</div>
            <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
              <defs></defs>
              <line x1="0.5" y1="4" x2="23.5" y2="4"></line>
              <line x1="0.232" y1="12" x2="23.5" y2="12"></line>
              <line x1="0.232" y1="20" x2="23.5" y2="20"></line>
            </svg>
          </label>
        </div>
        <div class="farama-header__left farama-header__center--mobile">
          <a href="../../../">
              <img class="farama-header__logo only-light" src="../../../_static/img/PettingZoo.svg" alt="Light Logo"/>
              <img class="farama-header__logo only-dark" src="../../../_static/img/PettingZoo_White.svg" alt="Dark Logo"/>
            <span class="farama-header__title">PettingZoo Documentation</span>
          </a>
        </div>
        <div class="farama-header__right">
          <div class="farama-header-menu">
            <button class="farama-header-menu__btn" aria-label="Open Farama Menu" aria-expanded="false" aria-haspopup="true" aria-controls="farama-menu">
              <img class="farama-black-logo-invert" src="../../../_static/img/farama-logo-header.svg">
              <svg viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <polyline style="stroke-linecap: round; stroke-linejoin: round; fill: none; stroke-width: 2px;" points="1 7 12 18 23 7"></polyline>
              </svg>
            </button>
            <div class="farama-header-menu-container farama-hidden" aria-hidden="true" id="farama-menu">
              <div class="farama-header-menu__header">
                <a href="https://farama.org">
                  <img class="farama-header-menu__logo farama-white-logo-invert" src="../../../_static/img/farama_solid_white.svg" alt="Farama Foundation logo">
                  <span>Farama Foundation</span>
                </a>
                <div class="farama-header-menu-header__right">
                  <button id="farama-close-menu">
                    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor"
                      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-close">
                      <line x1="3" y1="21" x2="21" y2="3"></line>
                      <line x1="3" y1="3" x2="21" y2="21"></line>
                    </svg>
                  </button>
                </div>
              </div>
              <div class="farama-header-menu__body">
                <!-- Response from farama.org/api/projects.json -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>

    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<div class="page">
  <!--<header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../"><div class="brand">PettingZoo Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>-->
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="farama-sidebar__title" href="../../../">
      <img class="farama-header__logo only-light" src="../../../_static/img/PettingZoo.svg" alt="Light Logo"/>
      <img class="farama-header__logo only-dark" src="../../../_static/img/PettingZoo_White.svg" alt="Dark Logo"/>
    <span class="farama-header__title">PettingZoo Documentation</span>
  </a><form class="sidebar-search-container" method="get" action="../../../search/" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../content/basic_usage/">Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../content/environment_creation/">Environment Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../content/environment_tests/">Testing Environments</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/aec/">AEC API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/parallel/">Parallel API</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api/wrappers/">Wrappers</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Wrappers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/pz_wrappers/">PettingZoo Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/supersuit_wrappers/">Supersuit Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/shimmy_wrappers/">Shimmy Compatibility Wrappers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/">Utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environments</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/atari/">Atari</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Atari</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/basketball_pong/">Basketball Pong</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/boxing/">Boxing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/combat_plane/">Combat: Plane</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/combat_tank/">Combat: Tank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/double_dunk/">Double Dunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/entombed_competitive/">Emtombed: Competitive</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/entombed_cooperative/">Emtombed: Cooperative</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/flag_capture/">Flag Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/foozpong/">Foozpong</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/ice_hockey/">Ice Hockey</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/joust/">Joust</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/mario_bros/">Mario Bros</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/maze_craze/">Maze Craze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/othello/">Othello</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/pong/">Pong</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/quadrapong/">Quadrapong</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/space_invaders/">Space Invaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/space_war/">Space War</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/surround/">Surround</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/tennis/">Tennis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/video_checkers/">Video Checkers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/volleyball_pong/">Volleyball Pong</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/warlords/">Warlords</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/atari/wizard_of_wor/">Wizard of Wor</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/butterfly/">Butterfly</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Butterfly</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/butterfly/cooperative_pong/">Cooperative Pong</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/butterfly/knights_archers_zombies/">Knights Archers Zombies (‘KAZ’)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/butterfly/pistonball/">Pistonball</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/classic/">Classic</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Classic</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/chess/">Chess</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/connect_four/">Connect Four</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/gin_rummy/">Gin Rummy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/go/">Go</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/hanabi/">Hanabi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/leduc_holdem/">Leduc Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/rps/">Rock Paper Scissors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/texas_holdem_no_limit/">Texas Hold’em No Limit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/texas_holdem/">Texas Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic/tictactoe/">Tic Tac Toe</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/mpe/">MPE</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of MPE</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple/">Simple</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_adversary/">Simple Adversary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_crypto/">Simple Crypto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_push/">Simple Push</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_reference/">Simple Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_speaker_listener/">Simple Speaker Listener</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_spread/">Simple Spread</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_tag/">Simple Tag</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mpe/simple_world_comm/">Simple World Comm</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/sisl/">SISL</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of SISL</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/sisl/multiwalker/">Multiwalker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/sisl/pursuit/">Pursuit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/sisl/waterworld/">Waterworld</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../environments/third_party_envs/">Third-Party Environments</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../custom_environment/">Custom Environment Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of Custom Environment Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../custom_environment/1-project-structure/">Tutorial: Repository Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../custom_environment/2-environment-logic/">Tutorial: Environment Logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../custom_environment/3-action-masking/">Tutorial: Action Masking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../custom_environment/4-testing-your-environment/">Tutorial: Testing Your Environment</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../cleanrl/">CleanRL Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of CleanRL Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cleanrl/implementing_PPO/">CleanRL: Implementing PPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cleanrl/advanced_PPO/">CleanRL: Advanced PPO</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tianshou/">Tianshou Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of Tianshou Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tianshou/beginner/">Tianshou: Basic API Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tianshou/intermediate/">Tianshou: Training Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tianshou/advanced/">Tianshou: CLI and Logging</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../rllib/">Ray RLlib Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of Ray RLlib Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../rllib/pistonball/">RLlib: PPO for Pistonball</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rllib/holdem/">RLlib: DQN for Simple Poker</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../langchain/">LangChain Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of LangChain Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../langchain/langchain/">LangChain: Creating LLM agents</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../sb3/">Stable-Baselines3 Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of Stable-Baselines3 Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sb3/kaz/">SB3: PPO for Knights-Archers-Zombies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sb3/waterworld/">SB3: PPO for Waterworld</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sb3/connect_four/">SB3: Action Masked PPO for Connect Four</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../">AgileRL Tutorial</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of AgileRL Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">AgileRL: Implementing DQN - Curriculum Learning and Self-play</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MADDPG/">AgileRL: Implementing MADDPG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MATD3/">AgileRL: Implementing MATD3</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Farama-Foundation/PettingZoo">Github</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../release_notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Farama-Foundation/PettingZoo/tree/master/docs/">Contribute to the Docs</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main-container">

    

    

    <div class="main">
      <div class="content">
        <div class="article-container">
          <a href="#" class="back-to-top muted-link">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
              <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
            </svg>
            <span>Back to top</span>
          </a>
          <div class="content-icon-container">
      <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/Farama-Foundation/PettingZoo/edit/master/docs/tutorials/agilerl/DQN.py" title="Edit this page">
    <svg aria-hidden="true" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4" />
      <line x1="13.5" y1="6.5" x2="17.5" y2="10.5" />
    </svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
              <button class="theme-toggle" title="Toggle color theme">
                <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                <svg class="theme-icon-when-auto">
                  <use href="#svg-sun-half"></use>
                </svg>
                <svg class="theme-icon-when-dark">
                  <use href="#svg-moon"></use>
                </svg>
                <svg class="theme-icon-when-light">
                  <use href="#svg-sun"></use>
                </svg>
              </button>
            </div>
            <label class="toc-overlay-icon toc-content-icon" for="__toc">
              <div class="visually-hidden">Toggle table of contents sidebar</div>
              <i class="icon"><svg>
                  <use href="#svg-toc"></use>
                </svg></i>
            </label>
          </div>
          <article role="main">
            
            <section class="tex2jax_ignore mathjax_ignore" id="agilerl-implementing-dqn-curriculum-learning-and-self-play">
<h1>AgileRL: Implementing DQN - Curriculum Learning and Self-play<a class="headerlink" href="#agilerl-implementing-dqn-curriculum-learning-and-self-play" title="Link to this heading">¶</a></h1>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../../../_images/connect_four_self_opp.gif"><img alt="../../../_images/connect_four_self_opp.gif" src="../../../_images/connect_four_self_opp.gif" style="height: 400px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig1: Agent trained to play Connect Four through self-play</span><a class="headerlink" href="#id1" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>This tutorial shows how to train a <a class="reference external" href="https://agilerl.readthedocs.io/en/latest/api/algorithms/dqn.html">DQN</a> agent on the <a class="reference external" href="https://pettingzoo.farama.org/environments/classic/connect_four/">connect four</a> classic environment.</p>
<p>This tutorial focuses on two techniques used in reinforcement learning - <strong>curriculum learning</strong> and <strong>self-play</strong>. Curriculum learning refers to training an agent on tasks of increasing difficulty in separate ‘lessons’. Imagine you were trying to become a chess world champion. You would not decide to learn to play chess by immediately taking on a grand master - it would be too difficult. Instead, you would practice against people of the same ability as you, improve slowly, and increasingly play against harder opponents until you were ready to compete with the best. The same concept applies to reinforcement learning models. Sometimes, tasks are too difficult to learn in one go, and so we must create a curriculum to guide an agent and teach it to solve our ultimate hard environment.</p>
<p>This tutorial also uses self-play. Self-play is a technique used in competitive reinforcement learning environments. An agent trains by playing against a copy of itself - the opponent - and learns to beat this opponent. The opponent is then updated to a copy of this better version of the agent, and the agent must then learn to beat itself again. This is done repeatedly, and the agent iteratively improves by exploiting its own weaknesses and discovering new strategies.</p>
<p>In this tutorial, self-play is treated as the final lesson in the curriculum. However, these two techniques can be used independently of each other, and with unlimited resources, self-play can beat agents trained with human-crafted lessons through curriculum learning. <a class="reference external" href="http://incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a> by Richard Sutton provides an interesting take on curriculum learning and is definitely worth consideration from any engineer undertaking such a task. However, unlike Sutton, we do not all have the resources available to us that Deepmind and top institutions provide, and so one must be pragmatic when deciding how they will solve their own reinforcement learning problem. If you would like to discuss this exciting area of research further, please join the AgileRL <a class="reference external" href="https://discord.com/invite/eB8HyTA2ux">Discord server</a> and let us know what you think!</p>
<section id="what-is-dqn">
<h2>What is DQN?<a class="headerlink" href="#what-is-dqn" title="Link to this heading">¶</a></h2>
<p><a class="reference external" href="https://agilerl.readthedocs.io/en/latest/api/algorithms/dqn.html">DQN</a> (Deep Q-Network) is an extension of Q-learning that makes use of a replay buffer and target network to improve learning stability. For further information on DQN, check out the AgileRL <a class="reference external" href="https://agilerl.readthedocs.io/en/latest/api/algorithms/dqn.html">documentation</a>.</p>
<section id="can-i-use-it">
<h3>Can I use it?<a class="headerlink" href="#can-i-use-it" title="Link to this heading">¶</a></h3>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Action Space</p></th>
<th class="head"><p>Observation Space</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Discrete</p></td>
<td><p>✔️</p></td>
<td><p>✔️</p></td>
</tr>
<tr class="row-odd"><td><p>Continuous</p></td>
<td><p>❌</p></td>
<td><p>✔️</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="environment-setup">
<h2>Environment Setup<a class="headerlink" href="#environment-setup" title="Link to this heading">¶</a></h2>
<p>To follow this tutorial, you will need to install the dependencies shown below. It is recommended to use a newly-created virtual environment to avoid dependency conflicts.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>agilerl==0.1.22; python_version &gt;= &#39;3.9&#39;
pettingzoo[classic,atari,mpe]&gt;=1.23.1
SuperSuit&gt;=3.9.0
torch&gt;=2.0.1
numpy&gt;=1.24.2
tqdm&gt;=4.65.0
fastrand==1.3.0
gymnasium&gt;=0.28.1
imageio&gt;=2.31.1
Pillow&gt;=9.5.0
PyYAML&gt;=5.4.1
wandb&gt;=0.13.10
</pre></div>
</div>
</section>
<section id="code">
<h2>Code<a class="headerlink" href="#code" title="Link to this heading">¶</a></h2>
<section id="curriculum-learning-and-self-play-using-dqn-on-connect-four">
<h3>Curriculum learning and self-play using DQN on Connect Four<a class="headerlink" href="#curriculum-learning-and-self-play-using-dqn-on-connect-four" title="Link to this heading">¶</a></h3>
<p>The following code should run without any issues. The comments are designed to help you understand how to use PettingZoo with AgileRL. If you have any questions, please feel free to ask in the <a class="reference external" href="https://discord.com/invite/eB8HyTA2ux">Discord server</a>.</p>
<p>This is a complicated tutorial, and so we will go through it in stages. The <a class="reference internal" href="#full-training-code">full code</a> can be found at the end of this section. Although much of this tutorial contains content specific to the Connect Four environment, it serves to demonstrate how techniques can be applied more generally to other problems.</p>
</section>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Link to this heading">¶</a></h3>
<p>Importing the following packages, functions and classes will enable us to run the tutorial.</p>
<details>
   <summary>Imports</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">from</span> <span class="nn">agilerl.components.replay_buffer</span> <span class="kn">import</span> <span class="n">ReplayBuffer</span>
<span class="kn">from</span> <span class="nn">agilerl.hpo.mutation</span> <span class="kn">import</span> <span class="n">Mutations</span>
<span class="kn">from</span> <span class="nn">agilerl.hpo.tournament</span> <span class="kn">import</span> <span class="n">TournamentSelection</span>
<span class="kn">from</span> <span class="nn">agilerl.utils.utils</span> <span class="kn">import</span> <span class="n">initialPopulation</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span><span class="p">,</span> <span class="n">trange</span>

<span class="kn">from</span> <span class="nn">pettingzoo.classic</span> <span class="kn">import</span> <span class="n">connect_four_v3</span>
</pre></div>
</div>
</details>
</section>
<section id="curriculum-learning">
<h3>Curriculum Learning<a class="headerlink" href="#curriculum-learning" title="Link to this heading">¶</a></h3>
<p>First, we need to set up and modify our environment to enable curriculum learning. Curriculum learning is enabled by changing the environment that the agent trains in. This can be implemented by changing what happens when certain actions are taken - altering the next observation returned by the environment, or more simply by altering the reward. First, we will change the reward. By default, Connect Four uses the following rewards:</p>
<ul class="simple">
<li><p>Win = +1</p></li>
<li><p>Lose = -1</p></li>
<li><p>Play continues = 0</p></li>
</ul>
<p>To help guide our agent, we can introduce rewards for other outcomes in the environment, such as a small reward for placing 3 pieces in a row, or a small negative reward when the opponent manages the same feat. We can also use reward shaping to encourage our agent to explore more. In Connect Four, if playing against a random opponent, an easy way to win is to always play in the same column. An agent may find success doing this, and therefore not learn other, more sophisticated strategies that can help it win against better opponents. We may therefore elect to reward vertical wins slightly less than horizontal or diagonal wins, to encourage the agent to try winning in different ways. An example reward system could be defined as follows:</p>
<ul class="simple">
<li><p>Win (horizontal or diagonal) = +1</p></li>
<li><p>Win (vertical) = +0.8</p></li>
<li><p>Three in a row = +0.05</p></li>
<li><p>Opponent three in a row = -0.05</p></li>
<li><p>Lose = -1</p></li>
<li><p>Play continues = 0</p></li>
</ul>
<section id="config-files">
<h4>Config files<a class="headerlink" href="#config-files" title="Link to this heading">¶</a></h4>
<p>It is best to use YAML config files to define the lessons in our curriculum and easily change and keep track of our settings. The first three lessons in our curriculum can be defined as follows:</p>
<details>
   <summary>Lesson 1</summary>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="c1"># Connect Four Lesson 1</span>
<span class="c1"># Train against random agent: &#39;random&#39;, weak opponent: &#39;weak&#39;, strong opponent: &#39;strong&#39;, or use self-play: &#39;self&#39;</span>
<span class="nt">opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">random</span>
<span class="nt">opponent_pool_size</span><span class="p">:</span><span class="w">       </span><span class="c1"># Size of opponent pool for self-play</span>
<span class="nt">opponent_upgrade</span><span class="p">:</span><span class="w">       </span><span class="c1"># Epoch frequency to update opponent pool</span>
<span class="nt">eval_opponent</span><span class="p">:</span><span class="w">       </span><span class="c1"># &#39;random&#39;, &#39;weak&#39; or &#39;strong&#39;</span>
<span class="nt">pretrained_path</span><span class="p">:</span><span class="w">       </span><span class="c1"># Path to pretrained model weights</span>
<span class="nt">save_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/DQN/lesson1_trained_agent.pt</span><span class="w">  </span><span class="c1"># Path to save trained model</span>
<span class="nt">max_train_episodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">  </span><span class="c1"># Maximum number of training episodes in environment</span>

<span class="c1">## Game specific:</span>
<span class="nt">buffer_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w">  </span><span class="c1"># Fill replay buffer with random experiences</span>
<span class="nt">warm_up_opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">random</span><span class="w">  </span><span class="c1"># Difficulty level of warm up experiences</span>
<span class="nt">agent_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30000</span><span class="w">  </span><span class="c1"># Number of epochs to warm up agent by training on random experiences</span>
<span class="nt">block_vert_coef</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w">  </span><span class="c1"># How many times more likely to block vertically</span>
<span class="nt">rewards</span><span class="p">:</span><span class="w">  </span><span class="c1"># Rewards for different outcomes</span>
<span class="w">    </span><span class="nt">win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">vertical_win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.7</span>
<span class="w">    </span><span class="nt">three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span>
<span class="w">    </span><span class="nt">opp_three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-0.05</span>
<span class="w">    </span><span class="nt">lose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="nt">play_continues</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</pre></div>
</div>
</details>
<details>
   <summary>Lesson 2</summary>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="c1"># Connect Four Lesson 2</span>
<span class="c1"># Train against random agent: &#39;random&#39;, weak opponent: &#39;weak&#39;, strong opponent: &#39;strong&#39;, or use self-play: &#39;self&#39;</span>
<span class="nt">opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">weak</span>
<span class="nt">opponent_pool_size</span><span class="p">:</span><span class="w">       </span><span class="c1"># Size of opponent pool for self-play</span>
<span class="nt">opponent_upgrade</span><span class="p">:</span><span class="w">       </span><span class="c1"># Epoch frequency to update opponent pool</span>
<span class="nt">eval_opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">weak</span><span class="w">  </span><span class="c1"># &#39;random&#39;, &#39;weak&#39; or &#39;strong&#39;</span>
<span class="nt">pretrained_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/DQN/lesson1_trained_agent.pt</span><span class="w">  </span><span class="c1"># Path to pretrained model weights</span>
<span class="nt">save_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/DQN/lesson2_trained_agent.pt</span><span class="w">  </span><span class="c1"># Path to save trained model</span>
<span class="nt">max_train_episodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100000</span><span class="w">  </span><span class="c1"># Maximum number of training episodes in environment</span>

<span class="c1">## Game specific:</span>
<span class="nt">buffer_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w">  </span><span class="c1"># Fill replay buffer with random experiences</span>
<span class="nt">warm_up_opponent</span><span class="p">:</span><span class="w">       </span><span class="c1"># Difficulty level of warm up experiences</span>
<span class="nt">agent_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">  </span><span class="c1"># Number of epochs to warm up agent by training on random experiences</span>
<span class="nt">block_vert_coef</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">  </span><span class="c1"># How many times more likely to block vertically</span>
<span class="nt">rewards</span><span class="p">:</span><span class="w">  </span><span class="c1"># Rewards for different outcomes</span>
<span class="w">    </span><span class="nt">win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">vertical_win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.02</span>
<span class="w">    </span><span class="nt">opp_three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-0.02</span>
<span class="w">    </span><span class="nt">lose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="nt">play_continues</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</pre></div>
</div>
</details>
<details>
   <summary>Lesson 3</summary>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="c1"># Connect Four Lesson 3</span>
<span class="c1"># Train against random agent: &#39;random&#39;, weak opponent: &#39;weak&#39;, strong opponent: &#39;strong&#39;, or use self-play: &#39;self&#39;</span>
<span class="nt">opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">strong</span>
<span class="nt">opponent_pool_size</span><span class="p">:</span><span class="w">       </span><span class="c1"># Size of opponent pool for self-play</span>
<span class="nt">opponent_upgrade</span><span class="p">:</span><span class="w">       </span><span class="c1"># Epoch frequency to update opponent pool</span>
<span class="nt">eval_opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">strong</span><span class="w">  </span><span class="c1"># &#39;random&#39;, &#39;weak&#39; or &#39;strong&#39;</span>
<span class="nt">pretrained_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/DQN/lesson2_trained_agent.pt</span><span class="w">   </span><span class="c1"># Path to pretrained model weights</span>
<span class="nt">save_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/DQN/lesson3_trained_agent.pt</span><span class="w">  </span><span class="c1"># Path to save trained model</span>
<span class="nt">max_train_episodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200000</span><span class="w">  </span><span class="c1"># Maximum number of training episodes in environment</span>

<span class="c1">## Game specific:</span>
<span class="nt">buffer_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w">  </span><span class="c1"># Fill replay buffer with random experiences</span>
<span class="nt">warm_up_opponent</span><span class="p">:</span><span class="w">  </span><span class="c1"># Difficulty level of warm up experiences</span>
<span class="nt">agent_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">  </span><span class="c1"># Number of epochs to warm up agent by training on random experiences</span>
<span class="nt">block_vert_coef</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">  </span><span class="c1"># How many times more likely to block vertically</span>
<span class="nt">rewards</span><span class="p">:</span><span class="w">  </span><span class="c1"># Rewards for different outcomes</span>
<span class="w">    </span><span class="nt">win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">vertical_win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.02</span>
<span class="w">    </span><span class="nt">opp_three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-0.02</span>
<span class="w">    </span><span class="nt">lose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="nt">play_continues</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</pre></div>
</div>
</details><br>
<p>To implement our curriculum, we create a <code class="docutils literal notranslate"><span class="pre">CurriculumEnv</span></code> class that acts as a wrapper on top of our Connect Four environment and enables us to alter the reward to guide the training of our agent. This uses the configs that we set up to define the lesson.</p>
<details>
   <summary>CurriculumEnv</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CurriculumEnv</span><span class="p">:</span>
<span class="w">   </span><span class="sd">&quot;&quot;&quot;Wrapper around environment to modify reward for curriculum learning.</span>

<span class="sd">   :param env: Environment to learn in</span>
<span class="sd">   :type env: PettingZoo-style environment</span>
<span class="sd">   :param lesson: Lesson settings for curriculum learning</span>
<span class="sd">   :type lesson: dict</span>
<span class="sd">   &quot;&quot;&quot;</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">lesson</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span> <span class="o">=</span> <span class="n">lesson</span>

   <span class="k">def</span> <span class="nf">fill_replay_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">opponent</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Fill the replay buffer with experiences collected by taking random actions in the environment.</span>

<span class="sd">      :param memory: Experience replay buffer</span>
<span class="sd">      :type memory: AgileRL experience replay buffer</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Filling replay buffer ...&quot;</span><span class="p">)</span>

      <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">memory</span><span class="o">.</span><span class="n">memory_size</span><span class="p">)</span>
      <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">memory</span><span class="o">.</span><span class="n">memory_size</span><span class="p">:</span>
         <span class="c1"># Randomly decide whether random player will go first or second</span>
         <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
               <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">False</span>
         <span class="k">else</span><span class="p">:</span>
               <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">True</span>

         <span class="n">mem_full</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># Reset environment at start of episode</span>
         <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

         <span class="p">(</span>
               <span class="n">p1_state</span><span class="p">,</span>
               <span class="n">p1_state_flipped</span><span class="p">,</span>
               <span class="n">p1_action</span><span class="p">,</span>
               <span class="n">p1_next_state</span><span class="p">,</span>
               <span class="n">p1_next_state_flipped</span><span class="p">,</span>
         <span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
         <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span>

         <span class="k">while</span> <span class="ow">not</span> <span class="p">(</span><span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">):</span>
               <span class="c1"># Player 0&#39;s turn</span>
               <span class="n">p0_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
               <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
               <span class="n">p0_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
               <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
               <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                  <span class="n">p0_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="s2">&quot;player_0&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">p0_action_mask</span><span class="p">)</span>
               <span class="k">else</span><span class="p">:</span>
                  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;warm_up_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                     <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                           <span class="n">p0_action_mask</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">]</span>
                     <span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                     <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p0_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
               <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
               <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
               <span class="n">p0_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
               <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

               <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                  <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                  <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                           <span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p1_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)</span>
                     <span class="p">),</span>
                     <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                     <span class="p">[</span>
                           <span class="n">reward</span><span class="p">,</span>
                           <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                           <span class="n">reward</span><span class="p">,</span>
                           <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                     <span class="p">],</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                           <span class="p">(</span>
                              <span class="n">p0_next_state</span><span class="p">,</span>
                              <span class="n">p1_next_state</span><span class="p">,</span>
                              <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                              <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                           <span class="p">)</span>
                     <span class="p">),</span>
                     <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                  <span class="p">)</span>
               <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                  <span class="k">if</span> <span class="n">p1_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                     <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                     <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p1_state</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)),</span>
                           <span class="p">[</span><span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                           <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="n">p1_next_state_flipped</span><span class="p">)),</span>
                           <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                     <span class="p">)</span>

                  <span class="c1"># Player 1&#39;s turn</span>
                  <span class="n">p1_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                  <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                  <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                  <span class="n">p1_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                  <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                  <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                     <span class="n">p1_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="s2">&quot;player_1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                           <span class="n">p1_action_mask</span>
                     <span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;warm_up_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                           <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                              <span class="n">p1_action_mask</span><span class="p">,</span> <span class="n">p0_action</span><span class="p">,</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">]</span>
                           <span class="p">)</span>
                     <span class="k">else</span><span class="p">:</span>
                           <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p1_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                  <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
                  <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                  <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                  <span class="n">p1_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                  <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                  <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                     <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                     <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                              <span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p1_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)</span>
                           <span class="p">),</span>
                           <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                           <span class="p">[</span>
                              <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                              <span class="n">reward</span><span class="p">,</span>
                              <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                              <span class="n">reward</span><span class="p">,</span>
                           <span class="p">],</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                              <span class="p">(</span>
                                 <span class="n">p0_next_state</span><span class="p">,</span>
                                 <span class="n">p1_next_state</span><span class="p">,</span>
                                 <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                                 <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                              <span class="p">)</span>
                           <span class="p">),</span>
                           <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                     <span class="p">)</span>

                  <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                     <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                     <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">)),</span>
                           <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">],</span>
                           <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="n">p0_next_state_flipped</span><span class="p">)),</span>
                           <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                     <span class="p">)</span>

         <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">-</span> <span class="n">mem_full</span><span class="p">)</span>
      <span class="n">pbar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Replay buffer warmed up.&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">memory</span>

   <span class="k">def</span> <span class="nf">check_winnable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lst</span><span class="p">,</span> <span class="n">piece</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Checks if four pieces in a row represent a winnable opportunity, e.g. [1, 1, 1, 0] or [2, 0, 2, 2].</span>

<span class="sd">      :param lst: List of pieces in row</span>
<span class="sd">      :type lst: List</span>
<span class="sd">      :param piece: Player piece we are checking (1 or 2)</span>
<span class="sd">      :type piece: int</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="n">lst</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">piece</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">lst</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

   <span class="k">def</span> <span class="nf">check_vertical_win</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Checks if a win is vertical.</span>

<span class="sd">      :param player: Player who we are checking, 0 or 1</span>
<span class="sd">      :type player: int</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
      <span class="n">piece</span> <span class="o">=</span> <span class="n">player</span> <span class="o">+</span> <span class="mi">1</span>

      <span class="n">column_count</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">row_count</span> <span class="o">=</span> <span class="mi">6</span>

      <span class="c1"># Check vertical locations for win</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span><span class="p">):</span>
         <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
               <span class="k">if</span> <span class="p">(</span>
                  <span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
                  <span class="ow">and</span> <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
                  <span class="ow">and</span> <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">2</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
                  <span class="ow">and</span> <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">3</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
               <span class="p">):</span>
                  <span class="k">return</span> <span class="kc">True</span>
      <span class="k">return</span> <span class="kc">False</span>

   <span class="k">def</span> <span class="nf">check_three_in_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Checks if there are three pieces in a row and a blank space next, or two pieces - blank - piece.</span>

<span class="sd">      :param player: Player who we are checking, 0 or 1</span>
<span class="sd">      :type player: int</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
      <span class="n">piece</span> <span class="o">=</span> <span class="n">player</span> <span class="o">+</span> <span class="mi">1</span>

      <span class="c1"># Check horizontal locations</span>
      <span class="n">column_count</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">row_count</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">three_in_row_count</span> <span class="o">=</span> <span class="mi">0</span>

      <span class="c1"># Check vertical locations</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span><span class="p">):</span>
         <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
               <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span><span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="p">:</span> <span class="n">r</span> <span class="o">+</span> <span class="mi">4</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">piece</span><span class="p">):</span>
                  <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

      <span class="c1"># Check horizontal locations</span>
      <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span><span class="p">):</span>
         <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
               <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span><span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="p">:</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">piece</span><span class="p">):</span>
                  <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

      <span class="c1"># Check positively sloped diagonals</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
         <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
               <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span>
                  <span class="p">[</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">2</span><span class="p">],</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">3</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">],</span>
                  <span class="n">piece</span><span class="p">,</span>
               <span class="p">):</span>
                  <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

      <span class="c1"># Check negatively sloped diagonals</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
         <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">row_count</span><span class="p">):</span>
               <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span>
                  <span class="p">[</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">2</span><span class="p">],</span>
                     <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">],</span>
                  <span class="n">piece</span><span class="p">,</span>
               <span class="p">):</span>
                  <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

      <span class="k">return</span> <span class="n">three_in_row_count</span>

   <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Processes and returns reward from environment according to lesson criteria.</span>

<span class="sd">      :param done: Environment has terminated</span>
<span class="sd">      :type done: bool</span>
<span class="sd">      :param player: Player who we are checking, 0 or 1</span>
<span class="sd">      :type player: int</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
         <span class="n">reward</span> <span class="o">=</span> <span class="p">(</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;vertical_win&quot;</span><span class="p">]</span>
               <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_vertical_win</span><span class="p">(</span><span class="n">player</span><span class="p">)</span>
               <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;win&quot;</span><span class="p">]</span>
         <span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
         <span class="n">agent_three_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_three_in_row</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">player</span><span class="p">)</span>
         <span class="n">opp_three_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_three_in_row</span><span class="p">(</span><span class="n">player</span><span class="p">)</span>
         <span class="k">if</span> <span class="p">(</span><span class="n">agent_three_count</span> <span class="o">+</span> <span class="n">opp_three_count</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
               <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;play_continues&quot;</span><span class="p">]</span>
         <span class="k">else</span><span class="p">:</span>
               <span class="n">reward</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;three_in_row&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">agent_three_count</span>
                  <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;opp_three_in_row&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">opp_three_count</span>
               <span class="p">)</span>
      <span class="k">return</span> <span class="n">reward</span>

   <span class="k">def</span> <span class="nf">last</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Wrapper around PettingZoo env last method.&quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

   <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Wrapper around PettingZoo env step method.&quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

   <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Wrapper around PettingZoo env reset method.&quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</details>
<p>When defining the different lessons in our curriculum, we can increase the difficulty of our task by modifying environment observations for our agent - in Connect Four, we can increase the skill level of our opponent. By progressively doing this, we can help our agent improve. We can change our rewards between lessons too; for example, we may wish to reward wins in all directions equally once we have learned to beat a random agent and now wish to train against a harder opponent. In this tutorial, an <code class="docutils literal notranslate"><span class="pre">Opponent</span></code> class is implemented to provide different levels of difficulty for training our agent.</p>
<details>
   <summary>Opponent</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Opponent</span><span class="p">:</span>
<span class="w">   </span><span class="sd">&quot;&quot;&quot;Connect 4 opponent to train and/or evaluate against.</span>

<span class="sd">   :param env: Environment to learn in</span>
<span class="sd">   :type env: PettingZoo-style environment</span>
<span class="sd">   :param difficulty: Difficulty level of opponent, &#39;random&#39;, &#39;weak&#39; or &#39;strong&#39;</span>
<span class="sd">   :type difficulty: str</span>
<span class="sd">   &quot;&quot;&quot;</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">difficulty</span> <span class="o">=</span> <span class="n">difficulty</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">difficulty</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">getAction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_opponent</span>
      <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">difficulty</span> <span class="o">==</span> <span class="s2">&quot;weak&quot;</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">getAction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weak_rule_based_opponent</span>
      <span class="k">else</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">getAction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strong_rule_based_opponent</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">length</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">top</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span>

   <span class="k">def</span> <span class="nf">update_top</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Updates self.top, a list which tracks the row on top of the highest piece in each column.&quot;&quot;&quot;</span>
      <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">)</span>
      <span class="n">non_zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">board</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">non_zeros</span>
      <span class="n">top</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">board</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">board</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
         <span class="n">column_pieces</span> <span class="o">=</span> <span class="n">rows</span><span class="p">[</span><span class="n">cols</span> <span class="o">==</span> <span class="n">col</span><span class="p">]</span>
         <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">column_pieces</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
               <span class="n">top</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">column_pieces</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
         <span class="k">else</span><span class="p">:</span>
               <span class="n">top</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">full_columns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">board</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">top</span><span class="p">[</span><span class="n">full_columns</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">top</span> <span class="o">=</span> <span class="n">top</span>

   <span class="k">def</span> <span class="nf">random_opponent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">,</span> <span class="n">last_opp_move</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">block_vert_coef</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Takes move for random opponent. If the lesson aims to randomly block vertical wins with a higher probability, this is done here too.</span>

<span class="sd">      :param action_mask: Mask of legal actions: 1=legal, 0=illegal</span>
<span class="sd">      :type action_mask: List</span>
<span class="sd">      :param last_opp_move: Most recent action taken by agent against this opponent</span>
<span class="sd">      :type last_opp_move: int</span>
<span class="sd">      :param block_vert_coef: How many times more likely to block vertically</span>
<span class="sd">      :type block_vert_coef: float</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">last_opp_move</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
         <span class="n">action_mask</span><span class="p">[</span><span class="n">last_opp_move</span><span class="p">]</span> <span class="o">*=</span> <span class="n">block_vert_coef</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">)),</span> <span class="n">action_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">action</span>

   <span class="k">def</span> <span class="nf">weak_rule_based_opponent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Takes move for weak rule-based opponent.</span>

<span class="sd">      :param player: Player who we are checking, 0 or 1</span>
<span class="sd">      :type player: int</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">update_top</span><span class="p">()</span>
      <span class="n">max_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
      <span class="n">best_actions</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">):</span>
         <span class="n">possible</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outcome</span><span class="p">(</span>
               <span class="n">action</span><span class="p">,</span> <span class="n">player</span><span class="p">,</span> <span class="n">return_length</span><span class="o">=</span><span class="kc">True</span>
         <span class="p">)</span>
         <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
               <span class="n">best_actions</span> <span class="o">=</span> <span class="p">[]</span>
               <span class="n">max_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
         <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">max_length</span><span class="p">:</span>
               <span class="n">best_actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">best_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">best_actions</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">best_action</span>

   <span class="k">def</span> <span class="nf">strong_rule_based_opponent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Takes move for strong rule-based opponent.</span>

<span class="sd">      :param player: Player who we are checking, 0 or 1</span>
<span class="sd">      :type player: int</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">update_top</span><span class="p">()</span>

      <span class="n">winning_actions</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">):</span>
         <span class="n">possible</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outcome</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">player</span><span class="p">)</span>
         <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">ended</span><span class="p">:</span>
               <span class="n">winning_actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">winning_actions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
         <span class="n">winning_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">winning_actions</span><span class="p">)</span>
         <span class="k">return</span> <span class="n">winning_action</span>

      <span class="n">opp</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">player</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
      <span class="n">loss_avoiding_actions</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">):</span>
         <span class="n">possible</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outcome</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">opp</span><span class="p">)</span>
         <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">ended</span><span class="p">:</span>
               <span class="n">loss_avoiding_actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_avoiding_actions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
         <span class="n">loss_avoiding_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">loss_avoiding_actions</span><span class="p">)</span>
         <span class="k">return</span> <span class="n">loss_avoiding_action</span>

      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weak_rule_based_opponent</span><span class="p">(</span><span class="n">player</span><span class="p">)</span>  <span class="c1"># take best possible move</span>

   <span class="k">def</span> <span class="nf">outcome</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">player</span><span class="p">,</span> <span class="n">return_length</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;Takes move for weak rule-based opponent.</span>

<span class="sd">      :param action: Action to take in environment</span>
<span class="sd">      :type action: int</span>
<span class="sd">      :param player: Player who we are checking, 0 or 1</span>
<span class="sd">      :type player: int</span>
<span class="sd">      :param return_length: Return length of outcomes, defaults to False</span>
<span class="sd">      :type player: bool, optional</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">top</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>  <span class="c1"># action column is full</span>
         <span class="k">return</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">+</span> <span class="p">((</span><span class="kc">None</span><span class="p">,)</span> <span class="k">if</span> <span class="n">return_length</span> <span class="k">else</span> <span class="p">())</span>

      <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">top</span><span class="p">[</span><span class="n">action</span><span class="p">],</span> <span class="n">action</span>
      <span class="n">piece</span> <span class="o">=</span> <span class="n">player</span> <span class="o">+</span> <span class="mi">1</span>

      <span class="c1"># down, up, left, right, down-left, up-right, down-right, up-left,</span>
      <span class="n">directions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
         <span class="p">[</span>
               <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
               <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
               <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
               <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
         <span class="p">]</span>
      <span class="p">)</span>  <span class="c1"># |4x2x2|</span>

      <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
         <span class="n">directions</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span>
      <span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">length</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
         <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
      <span class="p">)</span>  <span class="c1"># |4x2x3x2|</span>
      <span class="n">valid_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
         <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
               <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span>
         <span class="p">),</span>
         <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
               <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span>
         <span class="p">),</span>
      <span class="p">)</span>  <span class="c1"># |4x2x3|</span>
      <span class="n">d0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">valid_positions</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">d1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">valid_positions</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">)</span>
      <span class="n">board_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">valid_positions</span><span class="p">,</span> <span class="n">board</span><span class="p">[</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">board_values</span> <span class="o">==</span> <span class="n">piece</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
         <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
      <span class="p">)</span>  <span class="c1"># padding with zeros to compute length</span>
      <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

      <span class="n">ended</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="c1"># check if winnable in any direction</span>
      <span class="k">for</span> <span class="n">both_dir</span> <span class="ow">in</span> <span class="n">board_values</span><span class="p">:</span>
         <span class="c1"># |2x3|</span>
         <span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">both_dir</span><span class="p">[</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">piece</span><span class="p">],</span> <span class="n">both_dir</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
         <span class="k">if</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="p">[</span><span class="n">piece</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">length</span><span class="p">))</span> <span class="ow">in</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">line</span><span class="p">)):</span>
               <span class="n">ended</span> <span class="o">=</span> <span class="kc">True</span>
               <span class="k">break</span>

      <span class="c1"># ended = np.any(np.greater_equal(np.sum(lengths, 1), self.length - 1))</span>
      <span class="n">draw</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">top</span><span class="p">):</span>
         <span class="n">draw</span> <span class="o">&amp;=</span> <span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span> <span class="k">if</span> <span class="n">c</span> <span class="o">!=</span> <span class="n">col</span> <span class="k">else</span> <span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
      <span class="n">ended</span> <span class="o">|=</span> <span class="n">draw</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">player</span><span class="p">)</span> <span class="k">if</span> <span class="n">ended</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">draw</span> <span class="k">else</span> <span class="mi">0</span>

      <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span><span class="p">)</span> <span class="o">+</span> <span class="p">((</span><span class="n">lengths</span><span class="p">,)</span> <span class="k">if</span> <span class="n">return_length</span> <span class="k">else</span> <span class="p">())</span>

</pre></div>
</div>
</details>
</section>
</section>
<section id="general-setup">
<h3>General setup<a class="headerlink" href="#general-setup" title="Link to this heading">¶</a></h3>
<p>Before we go any further in this tutorial, it would be helpful to define and set up everything remaining we need for training.</p>
<details>
   <summary>Setup code</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;===== AgileRL Curriculum Learning Demo =====&quot;</span><span class="p">)</span>

<span class="n">lesson_number</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Load lesson for curriculum</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./curriculums/connect_four/lesson</span><span class="si">{</span><span class="n">lesson_number</span><span class="si">}</span><span class="s2">.yaml&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
   <span class="n">LESSON</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

<span class="c1"># Define the network configuration</span>
<span class="n">NET_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s2">&quot;arch&quot;</span><span class="p">:</span> <span class="s2">&quot;cnn&quot;</span><span class="p">,</span>  <span class="c1"># Network architecture</span>
   <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>  <span class="c1"># Actor hidden size</span>
   <span class="s2">&quot;channel_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">],</span>  <span class="c1"># CNN channel size</span>
   <span class="s2">&quot;kernel_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span>  <span class="c1"># CNN kernel size</span>
   <span class="s2">&quot;stride_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># CNN stride size</span>
   <span class="s2">&quot;normalize&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Normalize image from range [0,255] to [0,1]</span>
<span class="p">}</span>

<span class="c1"># Define the initial hyperparameters</span>
<span class="n">INIT_HP</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s2">&quot;POPULATION_SIZE&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
   <span class="c1"># &quot;ALGO&quot;: &quot;Rainbow DQN&quot;,  # Algorithm</span>
   <span class="s2">&quot;ALGO&quot;</span><span class="p">:</span> <span class="s2">&quot;DQN&quot;</span><span class="p">,</span>  <span class="c1"># Algorithm</span>
   <span class="s2">&quot;DOUBLE&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
   <span class="c1"># Swap image channels dimension from last to first [H, W, C] -&gt; [C, H, W]</span>
   <span class="s2">&quot;BATCH_SIZE&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>  <span class="c1"># Batch size</span>
   <span class="s2">&quot;LR&quot;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span>  <span class="c1"># Learning rate</span>
   <span class="s2">&quot;GAMMA&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>  <span class="c1"># Discount factor</span>
   <span class="s2">&quot;MEMORY_SIZE&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>  <span class="c1"># Max memory buffer size</span>
   <span class="s2">&quot;LEARN_STEP&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># Learning frequency</span>
   <span class="s2">&quot;N_STEP&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># Step number to calculate td error</span>
   <span class="s2">&quot;PER&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Use prioritized experience replay buffer</span>
   <span class="s2">&quot;ALPHA&quot;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>  <span class="c1"># Prioritized replay buffer parameter</span>
   <span class="s2">&quot;TAU&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>  <span class="c1"># For soft update of target parameters</span>
   <span class="s2">&quot;BETA&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>  <span class="c1"># Importance sampling coefficient</span>
   <span class="s2">&quot;PRIOR_EPS&quot;</span><span class="p">:</span> <span class="mf">0.000001</span><span class="p">,</span>  <span class="c1"># Minimum priority for sampling</span>
   <span class="s2">&quot;NUM_ATOMS&quot;</span><span class="p">:</span> <span class="mi">51</span><span class="p">,</span>  <span class="c1"># Unit number of support</span>
   <span class="s2">&quot;V_MIN&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>  <span class="c1"># Minimum value of support</span>
   <span class="s2">&quot;V_MAX&quot;</span><span class="p">:</span> <span class="mf">200.0</span><span class="p">,</span>  <span class="c1"># Maximum value of support</span>
   <span class="s2">&quot;WANDB&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Use Weights and Biases tracking</span>
<span class="p">}</span>

<span class="c1"># Define the connect four environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">connect_four_v3</span><span class="o">.</span><span class="n">env</span><span class="p">()</span>
<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c1"># Configure the algo input arguments</span>
<span class="n">state_dim</span> <span class="o">=</span> <span class="p">[</span>
   <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)[</span><span class="s2">&quot;observation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span>
<span class="p">]</span>
<span class="n">one_hot</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">action_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span><span class="o">.</span><span class="n">n</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">]</span>
<span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;DISCRETE_ACTIONS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;MAX_ACTION&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;MIN_ACTION&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># Warp the environment in the curriculum learning wrapper</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">CurriculumEnv</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">LESSON</span><span class="p">)</span>

<span class="c1"># Pre-process dimensions for PyTorch layers</span>
<span class="c1"># We only need to worry about the state dim of a single agent</span>
<span class="c1"># We flatten the 6x7x2 observation as input to the agent&quot;s neural network</span>
<span class="n">state_dim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span>
<span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Create a population ready for evolutionary hyper-parameter optimisation</span>
<span class="n">pop</span> <span class="o">=</span> <span class="n">initialPopulation</span><span class="p">(</span>
   <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;ALGO&quot;</span><span class="p">],</span>
   <span class="n">state_dim</span><span class="p">,</span>
   <span class="n">action_dim</span><span class="p">,</span>
   <span class="n">one_hot</span><span class="p">,</span>
   <span class="n">NET_CONFIG</span><span class="p">,</span>
   <span class="n">INIT_HP</span><span class="p">,</span>
   <span class="n">population_size</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;POPULATION_SIZE&quot;</span><span class="p">],</span>
   <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Configure the replay buffer</span>
<span class="n">field_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;next_state&quot;</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">]</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span>
   <span class="n">action_dim</span><span class="o">=</span><span class="n">action_dim</span><span class="p">,</span>  <span class="c1"># Number of agent actions</span>
   <span class="n">memory_size</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;MEMORY_SIZE&quot;</span><span class="p">],</span>  <span class="c1"># Max replay buffer size</span>
   <span class="n">field_names</span><span class="o">=</span><span class="n">field_names</span><span class="p">,</span>  <span class="c1"># Field names to store in memory</span>
   <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Instantiate a tournament selection object (used for HPO)</span>
<span class="n">tournament</span> <span class="o">=</span> <span class="n">TournamentSelection</span><span class="p">(</span>
   <span class="n">tournament_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Tournament selection size</span>
   <span class="n">elitism</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Elitism in tournament selection</span>
   <span class="n">population_size</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;POPULATION_SIZE&quot;</span><span class="p">],</span>  <span class="c1"># Population size</span>
   <span class="n">evo_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># Evaluate using last N fitness scores</span>

<span class="c1"># Instantiate a mutations object (used for HPO)</span>
<span class="n">mutations</span> <span class="o">=</span> <span class="n">Mutations</span><span class="p">(</span>
   <span class="n">algo</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;ALGO&quot;</span><span class="p">],</span>
   <span class="n">no_mutation</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of no mutation</span>
   <span class="n">architecture</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Probability of architecture mutation</span>
   <span class="n">new_layer_prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of new layer mutation</span>
   <span class="n">parameters</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of parameter mutation</span>
   <span class="n">activation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Probability of activation function mutation</span>
   <span class="n">rl_hp</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of RL hyperparameter mutation</span>
   <span class="n">rl_hp_selection</span><span class="o">=</span><span class="p">[</span>
         <span class="s2">&quot;lr&quot;</span><span class="p">,</span>
         <span class="s2">&quot;learn_step&quot;</span><span class="p">,</span>
         <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
   <span class="p">],</span>  <span class="c1"># RL hyperparams selected for mutation</span>
   <span class="n">mutation_sd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># Mutation strength</span>
   <span class="c1"># Define search space for each hyperparameter</span>
   <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
   <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
   <span class="n">min_learn_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
   <span class="n">max_learn_step</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
   <span class="n">min_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
   <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
   <span class="n">arch</span><span class="o">=</span><span class="n">NET_CONFIG</span><span class="p">[</span><span class="s2">&quot;arch&quot;</span><span class="p">],</span>  <span class="c1"># MLP or CNN</span>
   <span class="n">rand_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
   <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define training loop parameters</span>
<span class="n">episodes_per_epoch</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_episodes</span> <span class="o">=</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;max_train_episodes&quot;</span><span class="p">]</span>  <span class="c1"># Total episodes</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Maximum steps to take in each episode</span>
<span class="n">evo_epochs</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Evolution frequency</span>
<span class="n">evo_loop</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># Number of evaluation episodes</span>
<span class="n">elite</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Assign a placeholder &quot;elite&quot; agent</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Starting epsilon value</span>
<span class="n">eps_end</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Final epsilon value</span>
<span class="n">eps_decay</span> <span class="o">=</span> <span class="mf">0.9998</span>  <span class="c1"># Epsilon decays</span>
<span class="n">opp_update_counter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">wb</span> <span class="o">=</span> <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;WANDB&quot;</span><span class="p">]</span>

</pre></div>
</div>
</details>
<p>As part of the curriculum, we may also choose to fill the replay buffer with random experiences, and also train on these offline.</p>
<details>
   <summary>Fill replay buffer</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform buffer and agent warmups if desired</span>
<span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;buffer_warm_up&quot;</span><span class="p">]:</span>
   <span class="n">warm_up_opponent</span> <span class="o">=</span> <span class="n">Opponent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;warm_up_opponent&quot;</span><span class="p">])</span>
   <span class="n">memory</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fill_replay_buffer</span><span class="p">(</span>
         <span class="n">memory</span><span class="p">,</span> <span class="n">warm_up_opponent</span>
   <span class="p">)</span>  <span class="c1"># Fill replay buffer with transitions</span>
   <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;agent_warm_up&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
         <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warming up agents ...&quot;</span><span class="p">)</span>
         <span class="n">agent</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
         <span class="c1"># Train on randomly collected samples</span>
         <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;agent_warm_up&quot;</span><span class="p">]):</span>
            <span class="n">experiences</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">experiences</span><span class="p">)</span>
         <span class="n">pop</span> <span class="o">=</span> <span class="p">[</span><span class="n">agent</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">]</span>
         <span class="n">elite</span> <span class="o">=</span> <span class="n">agent</span>
         <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Agent population warmed up.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
<section id="self-play">
<h3>Self-play<a class="headerlink" href="#self-play" title="Link to this heading">¶</a></h3>
<p>In this tutorial, we use self-play as the final lesson in our curriculum. By iteratively improving our agent and making it learn to win against itself, we can allow it to discover new strategies and achieve higher performance. The weights of our pretrained agent from an earlier lesson can be loaded to the population as follows:</p>
<details>
   <summary>Load pretrained weights</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;pretrained_path&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
   <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
         <span class="c1"># Load pretrained checkpoint</span>
         <span class="n">agent</span><span class="o">.</span><span class="n">loadCheckpoint</span><span class="p">(</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;pretrained_path&quot;</span><span class="p">])</span>
         <span class="c1"># Reinit optimizer for new task</span>
         <span class="n">agent</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;LR&quot;</span><span class="p">]</span>
         <span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">agent</span><span class="o">.</span><span class="n">lr</span>
         <span class="p">)</span>
</pre></div>
</div>
</details>
<p>To train against an old version of our agent, we create a pool of opponents. At training time, we randomly select an opponent from this pool. At regular intervals, we update the opponent pool by removing the oldest opponent and adding a copy of the latest version of our agent. This provides a balance between training against an increasingly difficult opponent and providing variety in the moves an opponent might make.</p>
<details>
   <summary>Create opponent pool</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
   <span class="c1"># Create initial pool of opponents</span>
   <span class="n">opponent_pool</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent_pool_size&quot;</span><span class="p">])</span>
   <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent_pool_size&quot;</span><span class="p">]):</span>
         <span class="n">opp</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
         <span class="n">opp</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
         <span class="n">opp</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
         <span class="n">opponent_pool</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">opp</span><span class="p">)</span>
</pre></div>
</div>
</details>
<p>A sample lesson config for self-play training could be defined as follows:</p>
<details>
   <summary>Lesson 4</summary>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="c1"># Connect Four Lesson 4</span>
<span class="c1"># Train against random agent: &#39;random&#39;, weak opponent: &#39;weak&#39;, strong opponent: &#39;strong&#39;, or use self-play: &#39;self&#39;</span>
<span class="nt">opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">self</span>
<span class="nt">opponent_pool_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w">  </span><span class="c1"># Size of opponent pool for self-play</span>
<span class="nt">opponent_upgrade</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6000</span><span class="w">  </span><span class="c1"># Epoch frequency to update opponent pool</span>
<span class="nt">eval_opponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">strong</span><span class="w">  </span><span class="c1"># &#39;random&#39;, &#39;weak&#39; or &#39;strong&#39;</span>
<span class="nt">pretrained_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/DQN/lesson3_trained_agent.pt</span><span class="w">  </span><span class="c1"># Path to pretrained model weights</span>
<span class="nt">save_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/DQN/lesson4_trained_agent.pt</span><span class="w">  </span><span class="c1"># Path to save trained model</span>
<span class="nt">max_train_episodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">600000</span><span class="w">  </span><span class="c1"># Maximum number of training episodes in environment</span>

<span class="c1">## Game specific:</span>
<span class="nt">buffer_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w">  </span><span class="c1"># Fill replay buffer with random experiences</span>
<span class="nt">warm_up_opponent</span><span class="p">:</span><span class="w">       </span><span class="c1"># Difficulty level of warm up experiences</span>
<span class="nt">agent_warm_up</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">  </span><span class="c1"># Number of epochs to warm up agent by training on random experiences</span>
<span class="nt">block_vert_coef</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">  </span><span class="c1"># How many times more likely to block vertically if playing random opponent</span>
<span class="nt">rewards</span><span class="p">:</span><span class="w">  </span><span class="c1"># Rewards for different outcomes</span>
<span class="w">    </span><span class="nt">win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">vertical_win</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.01</span>
<span class="w">    </span><span class="nt">opp_three_in_row</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-0.01</span>
<span class="w">    </span><span class="nt">lose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="nt">play_continues</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</pre></div>
</div>
</details>
<p>It could also be possible to train an agent through self-play only, without using any previous lessons in the curriculum. This would require significant training time, but could ultimately result in better performance than other methods, and could avoid some of the mistakes discussed in <a class="reference external" href="http://incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>.</p>
</section>
<section id="training-loop">
<h3>Training loop<a class="headerlink" href="#training-loop" title="Link to this heading">¶</a></h3>
<p>The Connect Four training loop must take into account that the agent only takes an action every other interaction with the environment (the opponent takes alternating turns). This must be considered when saving transitions to the replay buffer. Equally, we must wait for the outcome of the next player’s turn before determining what the reward should be for a transition. This is not a true Markov Decision Process for this reason, but we can still train a reinforcement learning agent reasonably successfully in these non-stationary conditions.</p>
<p>At regular intervals, we evaluate the performance, or ‘fitness’,  of the agents in our population, and do an evolutionary step. Those which perform best are more likely to become members of the next generation, and the hyperparameters and neural architectures of agents in the population are mutated. This evolution allows us to optimize hyperparameters and maximise the performance of our agents in a single training run.</p>
<details>
   <summary>Training loop</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">max_episodes</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
   <span class="k">if</span> <span class="n">wb</span><span class="p">:</span>
      <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="c1"># set the wandb project where this run will be logged</span>
            <span class="n">project</span><span class="o">=</span><span class="s2">&quot;AgileRL&quot;</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">-EvoHPO-</span><span class="si">{}</span><span class="s2">-</span><span class="si">{}</span><span class="s2">Opposition-CNN-</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
               <span class="s2">&quot;connect_four_v3&quot;</span><span class="p">,</span>
               <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;ALGO&quot;</span><span class="p">],</span>
               <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">],</span>
               <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%m</span><span class="si">%d</span><span class="s2">%Y%H%M%S&quot;</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="c1"># track hyperparameters and run metadata</span>
            <span class="n">config</span><span class="o">=</span><span class="p">{</span>
               <span class="s2">&quot;algo&quot;</span><span class="p">:</span> <span class="s2">&quot;Evo HPO Rainbow DQN&quot;</span><span class="p">,</span>
               <span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="s2">&quot;connect_four_v3&quot;</span><span class="p">,</span>
               <span class="s2">&quot;INIT_HP&quot;</span><span class="p">:</span> <span class="n">INIT_HP</span><span class="p">,</span>
               <span class="s2">&quot;lesson&quot;</span><span class="p">:</span> <span class="n">LESSON</span><span class="p">,</span>
            <span class="p">},</span>
      <span class="p">)</span>

<span class="n">total_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_episodes</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">pbar</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">max_episodes</span> <span class="o">/</span> <span class="n">episodes_per_epoch</span><span class="p">))</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">idx_epi</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
   <span class="n">turns_per_episode</span> <span class="o">=</span> <span class="p">[]</span>
   <span class="n">train_actions_hist</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">action_dim</span>
   <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>  <span class="c1"># Loop through population</span>
         <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes_per_epoch</span><span class="p">):</span>
            <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># Reset environment at start of episode</span>
            <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

            <span class="p">(</span>
               <span class="n">p1_state</span><span class="p">,</span>
               <span class="n">p1_state_flipped</span><span class="p">,</span>
               <span class="n">p1_action</span><span class="p">,</span>
               <span class="n">p1_next_state</span><span class="p">,</span>
               <span class="n">p1_next_state_flipped</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
               <span class="c1"># Randomly choose opponent from opponent pool if using self-play</span>
               <span class="n">opponent</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">opponent_pool</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
               <span class="c1"># Create opponent of desired difficulty</span>
               <span class="n">opponent</span> <span class="o">=</span> <span class="n">Opponent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">])</span>

            <span class="c1"># Randomly decide whether agent will go first or second</span>
            <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
               <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
               <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">turns</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Number of turns counter</span>

            <span class="k">for</span> <span class="n">idx_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
               <span class="c1"># Player 0&quot;s turn</span>
               <span class="n">p0_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
               <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
               <span class="n">p0_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
               <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

               <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                     <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                        <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                           <span class="n">p0_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p0_action_mask</span>
                        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                     <span class="k">elif</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                        <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                           <span class="n">p0_action_mask</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                     <span class="k">else</span><span class="p">:</span>
                        <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
               <span class="k">else</span><span class="p">:</span>
                     <span class="n">p0_action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                        <span class="n">p0_state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">p0_action_mask</span>
                     <span class="p">)[</span>
                        <span class="mi">0</span>
                     <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                     <span class="n">train_actions_hist</span><span class="p">[</span><span class="n">p0_action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

               <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p0_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
               <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
               <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                     <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
               <span class="p">)</span>
               <span class="n">p0_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span>
               <span class="p">)</span>
               <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

               <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                     <span class="n">score</span> <span class="o">+=</span> <span class="n">env_reward</span>
               <span class="n">turns</span> <span class="o">+=</span> <span class="mi">1</span>

               <span class="c1"># Check if game is over (Player 0 win)</span>
               <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                     <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                     <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                           <span class="p">(</span>
                                 <span class="n">p0_state</span><span class="p">,</span>
                                 <span class="n">p1_state</span><span class="p">,</span>
                                 <span class="n">p0_state_flipped</span><span class="p">,</span>
                                 <span class="n">p1_state_flipped</span><span class="p">,</span>
                           <span class="p">)</span>
                        <span class="p">),</span>
                        <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                        <span class="p">[</span>
                           <span class="n">reward</span><span class="p">,</span>
                           <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                           <span class="n">reward</span><span class="p">,</span>
                           <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                        <span class="p">],</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                           <span class="p">(</span>
                                 <span class="n">p0_next_state</span><span class="p">,</span>
                                 <span class="n">p1_next_state</span><span class="p">,</span>
                                 <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                                 <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                           <span class="p">)</span>
                        <span class="p">),</span>
                        <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                     <span class="p">)</span>
               <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                     <span class="k">if</span> <span class="n">p1_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p1_state</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)),</span>
                           <span class="p">[</span><span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                           <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                 <span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="n">p1_next_state_flipped</span><span class="p">)</span>
                           <span class="p">),</span>
                           <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                        <span class="p">)</span>

                     <span class="c1"># Player 1&quot;s turn</span>
                     <span class="n">p1_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                     <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                        <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                     <span class="p">)</span>
                     <span class="c1"># Swap pieces so that the agent always sees the board from the same perspective</span>
                     <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                     <span class="n">p1_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                     <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                     <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                           <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                 <span class="n">p1_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p1_action_mask</span>
                           <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="k">elif</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                           <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                 <span class="n">p1_action_mask</span><span class="p">,</span>
                                 <span class="n">p0_action</span><span class="p">,</span>
                                 <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">],</span>
                           <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                           <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                     <span class="k">else</span><span class="p">:</span>
                        <span class="n">p1_action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                           <span class="n">p1_state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">p1_action_mask</span>
                        <span class="p">)[</span>
                           <span class="mi">0</span>
                        <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                        <span class="n">train_actions_hist</span><span class="p">[</span><span class="n">p1_action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                     <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p1_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                     <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
                     <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                        <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                     <span class="p">)</span>
                     <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                     <span class="n">p1_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span>
                     <span class="p">)</span>
                     <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                     <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                        <span class="n">score</span> <span class="o">+=</span> <span class="n">env_reward</span>
                     <span class="n">turns</span> <span class="o">+=</span> <span class="mi">1</span>

                     <span class="c1"># Check if game is over (Player 1 win)</span>
                     <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                        <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                 <span class="p">(</span>
                                    <span class="n">p0_state</span><span class="p">,</span>
                                    <span class="n">p1_state</span><span class="p">,</span>
                                    <span class="n">p0_state_flipped</span><span class="p">,</span>
                                    <span class="n">p1_state_flipped</span><span class="p">,</span>
                                 <span class="p">)</span>
                           <span class="p">),</span>
                           <span class="p">[</span>
                                 <span class="n">p0_action</span><span class="p">,</span>
                                 <span class="n">p1_action</span><span class="p">,</span>
                                 <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span>
                                 <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">,</span>
                           <span class="p">],</span>
                           <span class="p">[</span>
                                 <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                 <span class="n">reward</span><span class="p">,</span>
                                 <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                 <span class="n">reward</span><span class="p">,</span>
                           <span class="p">],</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                 <span class="p">(</span>
                                    <span class="n">p0_next_state</span><span class="p">,</span>
                                    <span class="n">p1_next_state</span><span class="p">,</span>
                                    <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                                    <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                                 <span class="p">)</span>
                           <span class="p">),</span>
                           <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                        <span class="p">)</span>

                     <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                        <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                        <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">)),</span>
                           <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">],</span>
                           <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                 <span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="n">p0_next_state_flipped</span><span class="p">)</span>
                           <span class="p">),</span>
                           <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                        <span class="p">)</span>

               <span class="c1"># Learn according to learning frequency</span>
               <span class="k">if</span> <span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">counter</span> <span class="o">%</span> <span class="n">agent</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                     <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">agent</span><span class="o">.</span><span class="n">batch_size</span>
               <span class="p">):</span>
                     <span class="c1"># Sample replay buffer</span>
                     <span class="c1"># Learn according to agent&quot;s RL algorithm</span>
                     <span class="n">experiences</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
                     <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">experiences</span><span class="p">)</span>

               <span class="c1"># Stop episode if any agents have terminated</span>
               <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                     <span class="k">break</span>

            <span class="n">total_steps</span> <span class="o">+=</span> <span class="n">idx_step</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">total_episodes</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">turns_per_episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">turns</span><span class="p">)</span>
            <span class="c1"># Save the total episode reward</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
               <span class="k">if</span> <span class="p">(</span><span class="n">total_episodes</span> <span class="o">%</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent_upgrade&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                     <span class="p">(</span><span class="n">idx_epi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">evo_epochs</span>
               <span class="p">):</span>
                     <span class="n">elite_opp</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tournament</span><span class="o">.</span><span class="n">_elitism</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
                     <span class="n">elite_opp</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                     <span class="n">opponent_pool</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elite_opp</span><span class="p">)</span>
                     <span class="n">opp_update_counter</span> <span class="o">+=</span> <span class="mi">1</span>

         <span class="c1"># Update epsilon for exploration</span>
         <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">eps_end</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">eps_decay</span><span class="p">)</span>

   <span class="n">mean_turns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">turns_per_episode</span><span class="p">)</span>

   <span class="c1"># Now evolve population if necessary</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">idx_epi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">evo_epochs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
         <span class="c1"># Evaluate population vs random actions</span>
         <span class="n">fitnesses</span> <span class="o">=</span> <span class="p">[]</span>
         <span class="n">win_rates</span> <span class="o">=</span> <span class="p">[]</span>
         <span class="n">eval_actions_hist</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">action_dim</span>  <span class="c1"># Eval actions histogram</span>
         <span class="n">eval_turns</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Eval turns counter</span>
         <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
               <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
               <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evo_loop</span><span class="p">):</span>
                     <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># Reset environment at start of episode</span>
                     <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

                     <span class="n">player</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Tracker for which player&quot;s turn it is</span>

                     <span class="c1"># Create opponent of desired difficulty</span>
                     <span class="n">opponent</span> <span class="o">=</span> <span class="n">Opponent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;eval_opponent&quot;</span><span class="p">])</span>

                     <span class="c1"># Randomly decide whether agent will go first or second</span>
                     <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                        <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">False</span>
                     <span class="k">else</span><span class="p">:</span>
                        <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">True</span>

                     <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

                     <span class="k">for</span> <span class="n">idx_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
                        <span class="n">action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                        <span class="k">if</span> <span class="n">player</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                           <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                                 <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;eval_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                                    <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">action_mask</span><span class="p">)</span>
                                 <span class="k">else</span><span class="p">:</span>
                                    <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                           <span class="k">else</span><span class="p">:</span>
                                 <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                                    <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                                 <span class="p">)</span>
                                 <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                                 <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">)[</span>
                                    <span class="mi">0</span>
                                 <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                                 <span class="n">eval_actions_hist</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="k">if</span> <span class="n">player</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                           <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                                 <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;eval_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                                    <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">action_mask</span><span class="p">)</span>
                                 <span class="k">else</span><span class="p">:</span>
                                    <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                           <span class="k">else</span><span class="p">:</span>
                                 <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                                    <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                                 <span class="p">)</span>
                                 <span class="n">state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                                 <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                                 <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">)[</span>
                                    <span class="mi">0</span>
                                 <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                                 <span class="n">eval_actions_hist</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                        <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

                        <span class="k">if</span> <span class="p">(</span><span class="n">player</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">opponent_first</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                           <span class="n">player</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">opponent_first</span>
                        <span class="p">):</span>
                           <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>

                        <span class="n">eval_turns</span> <span class="o">+=</span> <span class="mi">1</span>

                        <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                           <span class="k">break</span>

                        <span class="n">player</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

                     <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">mean_fit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_fit</span><span class="p">)</span>
            <span class="n">fitnesses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_fit</span><span class="p">)</span>

         <span class="n">eval_turns</span> <span class="o">=</span> <span class="n">eval_turns</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span> <span class="o">/</span> <span class="n">evo_loop</span>

         <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix_str</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;    Train Mean Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="n">episodes_per_epoch</span><span class="p">:])</span><span class="si">}</span><span class="s2">   Train Mean Turns: </span><span class="si">{</span><span class="n">mean_turns</span><span class="si">}</span><span class="s2">   Eval Mean Fitness: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">)</span><span class="si">}</span><span class="s2">   Eval Best Fitness: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">)</span><span class="si">}</span><span class="s2">   Eval Mean Turns: </span><span class="si">{</span><span class="n">eval_turns</span><span class="si">}</span><span class="s2">   Total Steps: </span><span class="si">{</span><span class="n">total_steps</span><span class="si">}</span><span class="s2">&quot;</span>
         <span class="p">)</span>
         <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

         <span class="c1"># Format action histograms for visualisation</span>
         <span class="n">train_actions_hist</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">freq</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">train_actions_hist</span><span class="p">)</span> <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">train_actions_hist</span>
         <span class="p">]</span>
         <span class="n">eval_actions_hist</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">freq</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">eval_actions_hist</span><span class="p">)</span> <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">eval_actions_hist</span>
         <span class="p">]</span>
         <span class="n">train_actions_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sa">f</span><span class="s2">&quot;train/action_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">action</span>
            <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_actions_hist</span><span class="p">)</span>
         <span class="p">}</span>
         <span class="n">eval_actions_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sa">f</span><span class="s2">&quot;eval/action_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">action</span>
            <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">eval_actions_hist</span><span class="p">)</span>
         <span class="p">}</span>

         <span class="k">if</span> <span class="n">wb</span><span class="p">:</span>
            <span class="n">wandb_dict</span> <span class="o">=</span> <span class="p">{</span>
               <span class="s2">&quot;global_step&quot;</span><span class="p">:</span> <span class="n">total_steps</span><span class="p">,</span>
               <span class="s2">&quot;train/mean_score&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="n">episodes_per_epoch</span><span class="p">:]),</span>
               <span class="s2">&quot;train/mean_turns_per_game&quot;</span><span class="p">:</span> <span class="n">mean_turns</span><span class="p">,</span>
               <span class="s2">&quot;train/epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span>
               <span class="s2">&quot;train/opponent_updates&quot;</span><span class="p">:</span> <span class="n">opp_update_counter</span><span class="p">,</span>
               <span class="s2">&quot;eval/mean_fitness&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">),</span>
               <span class="s2">&quot;eval/best_fitness&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">),</span>
               <span class="s2">&quot;eval/mean_turns_per_game&quot;</span><span class="p">:</span> <span class="n">eval_turns</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">wandb_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">train_actions_dict</span><span class="p">)</span>
            <span class="n">wandb_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">eval_actions_dict</span><span class="p">)</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">wandb_dict</span><span class="p">)</span>

         <span class="c1"># Tournament selection and population mutation</span>
         <span class="n">elite</span><span class="p">,</span> <span class="n">pop</span> <span class="o">=</span> <span class="n">tournament</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
         <span class="n">pop</span> <span class="o">=</span> <span class="n">mutations</span><span class="o">.</span><span class="n">mutation</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>

<span class="k">if</span> <span class="n">max_episodes</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
   <span class="k">if</span> <span class="n">wb</span><span class="p">:</span>
      <span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>

<span class="c1"># Save the trained agent</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;save_path&quot;</span><span class="p">]</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">save_path</span><span class="p">),</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">elite</span><span class="o">.</span><span class="n">saveCheckpoint</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Elite agent saved to &#39;</span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
<section id="trained-model-weights">
<h3>Trained model weights<a class="headerlink" href="#trained-model-weights" title="Link to this heading">¶</a></h3>
<p>Trained model weights are provided at <code class="docutils literal notranslate"><span class="pre">PettingZoo/tutorials/AgileRL/models</span></code>. Take a look, train against these models, and see if you can beat them!</p>
</section>
<section id="watch-the-trained-agents-play">
<h3>Watch the trained agents play<a class="headerlink" href="#watch-the-trained-agents-play" title="Link to this heading">¶</a></h3>
<p>The following code allows you to load your saved DQN agent from the previous training block, test the agent’s performance, and then visualise a number of episodes as a gif.</p>
<details>
   <summary>Render trained agents</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">agilerl.algorithms.dqn</span> <span class="kn">import</span> <span class="n">DQN</span>
<span class="kn">from</span> <span class="nn">agilerl_dqn_curriculum</span> <span class="kn">import</span> <span class="n">Opponent</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">ImageDraw</span><span class="p">,</span> <span class="n">ImageFont</span>

<span class="kn">from</span> <span class="nn">pettingzoo.classic</span> <span class="kn">import</span> <span class="n">connect_four_v3</span>


<span class="c1"># Define function to return image</span>
<span class="k">def</span> <span class="nf">_label_with_episode_number</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">episode_num</span><span class="p">,</span> <span class="n">frame_no</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">drawer</span> <span class="o">=</span> <span class="n">ImageDraw</span><span class="o">.</span><span class="n">Draw</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
    <span class="n">text_color</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">)</span>
    <span class="n">font</span> <span class="o">=</span> <span class="n">ImageFont</span><span class="o">.</span><span class="n">truetype</span><span class="p">(</span><span class="s2">&quot;arial.ttf&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">drawer</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
        <span class="sa">f</span><span class="s2">&quot;Episode: </span><span class="si">{</span><span class="n">episode_num</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">     Frame: </span><span class="si">{</span><span class="n">frame_no</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">fill</span><span class="o">=</span><span class="n">text_color</span><span class="p">,</span>
        <span class="n">font</span><span class="o">=</span><span class="n">font</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">player</span> <span class="o">=</span> <span class="s2">&quot;Player 1&quot;</span>
        <span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">player</span> <span class="o">=</span> <span class="s2">&quot;Player 2&quot;</span>
        <span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">player</span> <span class="o">=</span> <span class="s2">&quot;Self-play&quot;</span>
        <span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">)</span>
    <span class="n">drawer</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="mi">700</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Agent: </span><span class="si">{</span><span class="n">player</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">font</span><span class="o">=</span><span class="n">font</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">im</span>


<span class="c1"># Resizes frames to make file size smaller</span>
<span class="k">def</span> <span class="nf">resize_frames</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">fraction</span><span class="p">):</span>
    <span class="n">resized_frames</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">frames</span><span class="p">:</span>
        <span class="n">new_width</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">width</span> <span class="o">*</span> <span class="n">fraction</span><span class="p">)</span>
        <span class="n">new_height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">height</span> <span class="o">*</span> <span class="n">fraction</span><span class="p">)</span>
        <span class="n">img_resized</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="n">new_width</span><span class="p">,</span> <span class="n">new_height</span><span class="p">))</span>
        <span class="n">resized_frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img_resized</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">resized_frames</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;./models/DQN/lesson3_trained_agent.pt&quot;</span>  <span class="c1"># Path to saved agent checkpoint</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">connect_four_v3</span><span class="o">.</span><span class="n">env</span><span class="p">(</span><span class="n">render_mode</span><span class="o">=</span><span class="s2">&quot;rgb_array&quot;</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="c1"># Configure the algo input arguments</span>
    <span class="n">state_dim</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)[</span><span class="s2">&quot;observation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span>
    <span class="p">]</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">action_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span><span class="o">.</span><span class="n">n</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">]</span>

    <span class="c1"># Pre-process dimensions for pytorch layers</span>
    <span class="c1"># We will use self-play, so we only need to worry about the state dim of a single agent</span>
    <span class="c1"># We flatten the 6x7x2 observation as input to the agent&#39;s neural network</span>
    <span class="n">state_dim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Instantiate an DQN object</span>
    <span class="n">dqn</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
        <span class="n">state_dim</span><span class="p">,</span>
        <span class="n">action_dim</span><span class="p">,</span>
        <span class="n">one_hot</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Load the saved algorithm into the DQN object</span>
    <span class="n">dqn</span><span class="o">.</span><span class="n">loadCheckpoint</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">opponent_difficulty</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="s2">&quot;weak&quot;</span><span class="p">,</span> <span class="s2">&quot;strong&quot;</span><span class="p">,</span> <span class="s2">&quot;self&quot;</span><span class="p">]:</span>
        <span class="c1"># Create opponent</span>
        <span class="k">if</span> <span class="n">opponent_difficulty</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
            <span class="n">opponent</span> <span class="o">=</span> <span class="n">dqn</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">opponent</span> <span class="o">=</span> <span class="n">Opponent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">opponent_difficulty</span><span class="p">)</span>

        <span class="c1"># Define test loop parameters</span>
        <span class="n">episodes</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Number of episodes to test agent on</span>
        <span class="n">max_steps</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">500</span>  <span class="c1"># Max number of steps to take in the environment in each episode</span>
        <span class="p">)</span>

        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># List to collect total episodic reward</span>
        <span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># List to collect frames</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Agent: </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Opponent: </span><span class="si">{</span><span class="n">opponent_difficulty</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Test loop for inference</span>
        <span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">ep</span> <span class="o">/</span> <span class="n">episodes</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
                <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">opponent_difficulty</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># Reset environment at start of episode</span>
            <span class="n">frame</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">_label_with_episode_number</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">episode_num</span><span class="o">=</span><span class="n">ep</span><span class="p">,</span> <span class="n">frame_no</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
            <span class="n">player</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Tracker for which player&#39;s turn it is</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">idx_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
                <span class="n">action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">player</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">opponent_difficulty</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="o">=</span><span class="n">action_mask</span>
                            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="k">elif</span> <span class="n">opponent_difficulty</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">action_mask</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">action</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                            <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="o">=</span><span class="n">action_mask</span>
                        <span class="p">)[</span>
                            <span class="mi">0</span>
                        <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                <span class="k">if</span> <span class="n">player</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                    <span class="n">state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">opponent_difficulty</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="o">=</span><span class="n">action_mask</span>
                            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="k">elif</span> <span class="n">opponent_difficulty</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">action_mask</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">action</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                            <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="o">=</span><span class="n">action_mask</span>
                        <span class="p">)[</span>
                            <span class="mi">0</span>
                        <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">termination</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
                <span class="c1"># Save the frame for this step and append to frames list</span>
                <span class="n">frame</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
                <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">_label_with_episode_number</span><span class="p">(</span>
                        <span class="n">frame</span><span class="p">,</span> <span class="n">episode_num</span><span class="o">=</span><span class="n">ep</span><span class="p">,</span> <span class="n">frame_no</span><span class="o">=</span><span class="n">idx_step</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span>
                    <span class="p">)</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="p">(</span><span class="n">player</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">opponent_first</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="n">player</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">opponent_first</span>
                <span class="p">):</span>
                    <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">score</span> <span class="o">-=</span> <span class="n">reward</span>

                <span class="c1"># Stop episode if any agents have terminated</span>
                <span class="k">if</span> <span class="n">truncation</span> <span class="ow">or</span> <span class="n">termination</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="n">player</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">15</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Episode: </span><span class="si">{</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">15</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode length: </span><span class="si">{</span><span class="n">idx_step</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Score: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================&quot;</span><span class="p">)</span>

        <span class="n">frames</span> <span class="o">=</span> <span class="n">resize_frames</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

        <span class="c1"># Save the gif to specified path</span>
        <span class="n">gif_path</span> <span class="o">=</span> <span class="s2">&quot;./videos/&quot;</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">gif_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">imageio</span><span class="o">.</span><span class="n">mimwrite</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;./videos/&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;connect_four_</span><span class="si">{</span><span class="n">opponent_difficulty</span><span class="si">}</span><span class="s2">_opp.gif&quot;</span><span class="p">),</span>
            <span class="n">frames</span><span class="p">,</span>
            <span class="n">duration</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
            <span class="n">loop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</details>
</section>
<section id="full-training-code">
<h3>Full training code<a class="headerlink" href="#full-training-code" title="Link to this heading">¶</a></h3>
<details>
   <summary>Full training code</summary>
<blockquote>
<div><p>Please note that on line 612 <code class="docutils literal notranslate"><span class="pre">max_episodes</span></code> is set to 10 to allow fast testing of this tutorial code. This line can be deleted, and the line below it uncommented, to use the number of episodes set in the config files.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;This tutorial shows how to train a DQN agent on the connect four environment, using curriculum learning and self play.</span>

<span class="sd">Author: Nick (https://github.com/nicku-a)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">from</span> <span class="nn">agilerl.components.replay_buffer</span> <span class="kn">import</span> <span class="n">ReplayBuffer</span>
<span class="kn">from</span> <span class="nn">agilerl.hpo.mutation</span> <span class="kn">import</span> <span class="n">Mutations</span>
<span class="kn">from</span> <span class="nn">agilerl.hpo.tournament</span> <span class="kn">import</span> <span class="n">TournamentSelection</span>
<span class="kn">from</span> <span class="nn">agilerl.utils.utils</span> <span class="kn">import</span> <span class="n">initialPopulation</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span><span class="p">,</span> <span class="n">trange</span>

<span class="kn">from</span> <span class="nn">pettingzoo.classic</span> <span class="kn">import</span> <span class="n">connect_four_v3</span>


<span class="k">class</span> <span class="nc">CurriculumEnv</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper around environment to modify reward for curriculum learning.</span>

<span class="sd">    :param env: Environment to learn in</span>
<span class="sd">    :type env: PettingZoo-style environment</span>
<span class="sd">    :param lesson: Lesson settings for curriculum learning</span>
<span class="sd">    :type lesson: dict</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">lesson</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span> <span class="o">=</span> <span class="n">lesson</span>

    <span class="k">def</span> <span class="nf">fill_replay_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">opponent</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fill the replay buffer with experiences collected by taking random actions in the environment.</span>

<span class="sd">        :param memory: Experience replay buffer</span>
<span class="sd">        :type memory: AgileRL experience replay buffer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Filling replay buffer ...&quot;</span><span class="p">)</span>

        <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">memory</span><span class="o">.</span><span class="n">memory_size</span><span class="p">)</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">memory</span><span class="o">.</span><span class="n">memory_size</span><span class="p">:</span>
            <span class="c1"># Randomly decide whether random player will go first or second</span>
            <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">mem_full</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># Reset environment at start of episode</span>
            <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

            <span class="p">(</span>
                <span class="n">p1_state</span><span class="p">,</span>
                <span class="n">p1_state_flipped</span><span class="p">,</span>
                <span class="n">p1_action</span><span class="p">,</span>
                <span class="n">p1_next_state</span><span class="p">,</span>
                <span class="n">p1_next_state_flipped</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span>

            <span class="k">while</span> <span class="ow">not</span> <span class="p">(</span><span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">):</span>
                <span class="c1"># Player 0&#39;s turn</span>
                <span class="n">p0_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                <span class="n">p0_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                    <span class="n">p0_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="s2">&quot;player_0&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">p0_action_mask</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;warm_up_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                        <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                            <span class="n">p0_action_mask</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p0_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
                <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                <span class="n">p0_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                    <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                            <span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p1_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)</span>
                        <span class="p">),</span>
                        <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                        <span class="p">[</span>
                            <span class="n">reward</span><span class="p">,</span>
                            <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                            <span class="n">reward</span><span class="p">,</span>
                            <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                        <span class="p">],</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                            <span class="p">(</span>
                                <span class="n">p0_next_state</span><span class="p">,</span>
                                <span class="n">p1_next_state</span><span class="p">,</span>
                                <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                                <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="p">),</span>
                        <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                    <span class="k">if</span> <span class="n">p1_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p1_state</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)),</span>
                            <span class="p">[</span><span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                            <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="n">p1_next_state_flipped</span><span class="p">)),</span>
                            <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                        <span class="p">)</span>

                    <span class="c1"># Player 1&#39;s turn</span>
                    <span class="n">p1_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                    <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                    <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                    <span class="n">p1_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                        <span class="n">p1_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="s2">&quot;player_1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                            <span class="n">p1_action_mask</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;warm_up_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                            <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                <span class="n">p1_action_mask</span><span class="p">,</span> <span class="n">p0_action</span><span class="p">,</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p1_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                    <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
                    <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                    <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                    <span class="n">p1_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                <span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p1_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)</span>
                            <span class="p">),</span>
                            <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                            <span class="p">[</span>
                                <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                <span class="n">reward</span><span class="p">,</span>
                                <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                <span class="n">reward</span><span class="p">,</span>
                            <span class="p">],</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                <span class="p">(</span>
                                    <span class="n">p0_next_state</span><span class="p">,</span>
                                    <span class="n">p1_next_state</span><span class="p">,</span>
                                    <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                                    <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                                <span class="p">)</span>
                            <span class="p">),</span>
                            <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                        <span class="p">)</span>

                    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                        <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">)),</span>
                            <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">],</span>
                            <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="n">p0_next_state_flipped</span><span class="p">)),</span>
                            <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                        <span class="p">)</span>

            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">-</span> <span class="n">mem_full</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Replay buffer warmed up.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">memory</span>

    <span class="k">def</span> <span class="nf">check_winnable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lst</span><span class="p">,</span> <span class="n">piece</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Checks if four pieces in a row represent a winnable opportunity, e.g. [1, 1, 1, 0] or [2, 0, 2, 2].</span>

<span class="sd">        :param lst: List of pieces in row</span>
<span class="sd">        :type lst: List</span>
<span class="sd">        :param piece: Player piece we are checking (1 or 2)</span>
<span class="sd">        :type piece: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">lst</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">piece</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">lst</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">check_vertical_win</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Checks if a win is vertical.</span>

<span class="sd">        :param player: Player who we are checking, 0 or 1</span>
<span class="sd">        :type player: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
        <span class="n">piece</span> <span class="o">=</span> <span class="n">player</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">column_count</span> <span class="o">=</span> <span class="mi">7</span>
        <span class="n">row_count</span> <span class="o">=</span> <span class="mi">6</span>

        <span class="c1"># Check vertical locations for win</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
                    <span class="ow">and</span> <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
                    <span class="ow">and</span> <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">2</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
                    <span class="ow">and</span> <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">3</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">==</span> <span class="n">piece</span>
                <span class="p">):</span>
                    <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">check_three_in_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Checks if there are three pieces in a row and a blank space next, or two pieces - blank - piece.</span>

<span class="sd">        :param player: Player who we are checking, 0 or 1</span>
<span class="sd">        :type player: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
        <span class="n">piece</span> <span class="o">=</span> <span class="n">player</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Check horizontal locations</span>
        <span class="n">column_count</span> <span class="o">=</span> <span class="mi">7</span>
        <span class="n">row_count</span> <span class="o">=</span> <span class="mi">6</span>
        <span class="n">three_in_row_count</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Check vertical locations</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span><span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="p">:</span> <span class="n">r</span> <span class="o">+</span> <span class="mi">4</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">piece</span><span class="p">):</span>
                    <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Check horizontal locations</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span><span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="p">:</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">piece</span><span class="p">):</span>
                    <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Check positively sloped diagonals</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="mi">3</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">3</span><span class="p">],</span>
                    <span class="p">],</span>
                    <span class="n">piece</span><span class="p">,</span>
                <span class="p">):</span>
                    <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Check negatively sloped diagonals</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">column_count</span> <span class="o">-</span> <span class="mi">3</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">row_count</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_winnable</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="n">board</span><span class="p">[</span><span class="n">r</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">3</span><span class="p">],</span>
                    <span class="p">],</span>
                    <span class="n">piece</span><span class="p">,</span>
                <span class="p">):</span>
                    <span class="n">three_in_row_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">three_in_row_count</span>

    <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Processes and returns reward from environment according to lesson criteria.</span>

<span class="sd">        :param done: Environment has terminated</span>
<span class="sd">        :type done: bool</span>
<span class="sd">        :param player: Player who we are checking, 0 or 1</span>
<span class="sd">        :type player: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;vertical_win&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_vertical_win</span><span class="p">(</span><span class="n">player</span><span class="p">)</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;win&quot;</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">agent_three_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_three_in_row</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">player</span><span class="p">)</span>
            <span class="n">opp_three_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_three_in_row</span><span class="p">(</span><span class="n">player</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">agent_three_count</span> <span class="o">+</span> <span class="n">opp_three_count</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;play_continues&quot;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;three_in_row&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">agent_three_count</span>
                    <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lesson</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;opp_three_in_row&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">opp_three_count</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">reward</span>

    <span class="k">def</span> <span class="nf">last</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wrapper around PettingZoo env last method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wrapper around PettingZoo env step method.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wrapper around PettingZoo env reset method.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Opponent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Connect 4 opponent to train and/or evaluate against.</span>

<span class="sd">    :param env: Environment to learn in</span>
<span class="sd">    :type env: PettingZoo-style environment</span>
<span class="sd">    :param difficulty: Difficulty level of opponent, &#39;random&#39;, &#39;weak&#39; or &#39;strong&#39;</span>
<span class="sd">    :type difficulty: str</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">difficulty</span> <span class="o">=</span> <span class="n">difficulty</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">difficulty</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">getAction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_opponent</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">difficulty</span> <span class="o">==</span> <span class="s2">&quot;weak&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">getAction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weak_rule_based_opponent</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">getAction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strong_rule_based_opponent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span> <span class="o">=</span> <span class="mi">7</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">=</span> <span class="mi">6</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">top</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span>

    <span class="k">def</span> <span class="nf">update_top</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updates self.top, a list which tracks the row on top of the highest piece in each column.&quot;&quot;&quot;</span>
        <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">)</span>
        <span class="n">non_zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">board</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">non_zeros</span>
        <span class="n">top</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">board</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">board</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">column_pieces</span> <span class="o">=</span> <span class="n">rows</span><span class="p">[</span><span class="n">cols</span> <span class="o">==</span> <span class="n">col</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">column_pieces</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">top</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">column_pieces</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">top</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">full_columns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">board</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">top</span><span class="p">[</span><span class="n">full_columns</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">top</span> <span class="o">=</span> <span class="n">top</span>

    <span class="k">def</span> <span class="nf">random_opponent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">,</span> <span class="n">last_opp_move</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">block_vert_coef</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Takes move for random opponent. If the lesson aims to randomly block vertical wins with a higher probability, this is done here too.</span>

<span class="sd">        :param action_mask: Mask of legal actions: 1=legal, 0=illegal</span>
<span class="sd">        :type action_mask: List</span>
<span class="sd">        :param last_opp_move: Most recent action taken by agent against this opponent</span>
<span class="sd">        :type last_opp_move: int</span>
<span class="sd">        :param block_vert_coef: How many times more likely to block vertically</span>
<span class="sd">        :type block_vert_coef: float</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">last_opp_move</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">action_mask</span><span class="p">[</span><span class="n">last_opp_move</span><span class="p">]</span> <span class="o">*=</span> <span class="n">block_vert_coef</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">)),</span> <span class="n">action_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">weak_rule_based_opponent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Takes move for weak rule-based opponent.</span>

<span class="sd">        :param player: Player who we are checking, 0 or 1</span>
<span class="sd">        :type player: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_top</span><span class="p">()</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">best_actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">):</span>
            <span class="n">possible</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outcome</span><span class="p">(</span>
                <span class="n">action</span><span class="p">,</span> <span class="n">player</span><span class="p">,</span> <span class="n">return_length</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
                <span class="n">best_actions</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">max_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">max_length</span><span class="p">:</span>
                <span class="n">best_actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">best_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">best_actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">best_action</span>

    <span class="k">def</span> <span class="nf">strong_rule_based_opponent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Takes move for strong rule-based opponent.</span>

<span class="sd">        :param player: Player who we are checking, 0 or 1</span>
<span class="sd">        :type player: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_top</span><span class="p">()</span>

        <span class="n">winning_actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">):</span>
            <span class="n">possible</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outcome</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">player</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">ended</span><span class="p">:</span>
                <span class="n">winning_actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">winning_actions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">winning_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">winning_actions</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">winning_action</span>

        <span class="n">opp</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">player</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">loss_avoiding_actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">):</span>
            <span class="n">possible</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outcome</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">opp</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">possible</span> <span class="ow">and</span> <span class="n">ended</span><span class="p">:</span>
                <span class="n">loss_avoiding_actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_avoiding_actions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss_avoiding_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">loss_avoiding_actions</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss_avoiding_action</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weak_rule_based_opponent</span><span class="p">(</span><span class="n">player</span><span class="p">)</span>  <span class="c1"># take best possible move</span>

    <span class="k">def</span> <span class="nf">outcome</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">player</span><span class="p">,</span> <span class="n">return_length</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Takes move for weak rule-based opponent.</span>

<span class="sd">        :param action: Action to take in environment</span>
<span class="sd">        :type action: int</span>
<span class="sd">        :param player: Player who we are checking, 0 or 1</span>
<span class="sd">        :type player: int</span>
<span class="sd">        :param return_length: Return length of outcomes, defaults to False</span>
<span class="sd">        :type player: bool, optional</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">top</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>  <span class="c1"># action column is full</span>
            <span class="k">return</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">+</span> <span class="p">((</span><span class="kc">None</span><span class="p">,)</span> <span class="k">if</span> <span class="n">return_length</span> <span class="k">else</span> <span class="p">())</span>

        <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">top</span><span class="p">[</span><span class="n">action</span><span class="p">],</span> <span class="n">action</span>
        <span class="n">piece</span> <span class="o">=</span> <span class="n">player</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># down, up, left, right, down-left, up-right, down-right, up-left,</span>
        <span class="n">directions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
                <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
                <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
                <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
            <span class="p">]</span>
        <span class="p">)</span>  <span class="c1"># |4x2x2|</span>

        <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">directions</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span>
        <span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">length</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># |4x2x3x2|</span>
        <span class="n">valid_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
                <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span>
            <span class="p">),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
                <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span>
            <span class="p">),</span>
        <span class="p">)</span>  <span class="c1"># |4x2x3|</span>
        <span class="n">d0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">valid_positions</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">valid_positions</span><span class="p">,</span> <span class="n">positions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cols</span><span class="p">)</span>
        <span class="n">board_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">valid_positions</span><span class="p">,</span> <span class="n">board</span><span class="p">[</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">board_values</span> <span class="o">==</span> <span class="n">piece</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># padding with zeros to compute length</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">ended</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># check if winnable in any direction</span>
        <span class="k">for</span> <span class="n">both_dir</span> <span class="ow">in</span> <span class="n">board_values</span><span class="p">:</span>
            <span class="c1"># |2x3|</span>
            <span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">both_dir</span><span class="p">[</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">piece</span><span class="p">],</span> <span class="n">both_dir</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">if</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="p">[</span><span class="n">piece</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">length</span><span class="p">))</span> <span class="ow">in</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">line</span><span class="p">)):</span>
                <span class="n">ended</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>

        <span class="c1"># ended = np.any(np.greater_equal(np.sum(lengths, 1), self.length - 1))</span>
        <span class="n">draw</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">top</span><span class="p">):</span>
            <span class="n">draw</span> <span class="o">&amp;=</span> <span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span> <span class="k">if</span> <span class="n">c</span> <span class="o">!=</span> <span class="n">col</span> <span class="k">else</span> <span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">ended</span> <span class="o">|=</span> <span class="n">draw</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">player</span><span class="p">)</span> <span class="k">if</span> <span class="n">ended</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">draw</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ended</span><span class="p">)</span> <span class="o">+</span> <span class="p">((</span><span class="n">lengths</span><span class="p">,)</span> <span class="k">if</span> <span class="n">return_length</span> <span class="k">else</span> <span class="p">())</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;===== AgileRL Curriculum Learning Demo =====&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">lesson_number</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
        <span class="c1"># Load lesson for curriculum</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./curriculums/connect_four/lesson</span><span class="si">{</span><span class="n">lesson_number</span><span class="si">}</span><span class="s2">.yaml&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">LESSON</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

        <span class="c1"># Define the network configuration</span>
        <span class="n">NET_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;arch&quot;</span><span class="p">:</span> <span class="s2">&quot;cnn&quot;</span><span class="p">,</span>  <span class="c1"># Network architecture</span>
            <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>  <span class="c1"># Actor hidden size</span>
            <span class="s2">&quot;channel_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">],</span>  <span class="c1"># CNN channel size</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span>  <span class="c1"># CNN kernel size</span>
            <span class="s2">&quot;stride_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># CNN stride size</span>
            <span class="s2">&quot;normalize&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Normalize image from range [0,255] to [0,1]</span>
        <span class="p">}</span>

        <span class="c1"># Define the initial hyperparameters</span>
        <span class="n">INIT_HP</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;POPULATION_SIZE&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
            <span class="c1"># &quot;ALGO&quot;: &quot;Rainbow DQN&quot;,  # Algorithm</span>
            <span class="s2">&quot;ALGO&quot;</span><span class="p">:</span> <span class="s2">&quot;DQN&quot;</span><span class="p">,</span>  <span class="c1"># Algorithm</span>
            <span class="s2">&quot;DOUBLE&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="c1"># Swap image channels dimension from last to first [H, W, C] -&gt; [C, H, W]</span>
            <span class="s2">&quot;BATCH_SIZE&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>  <span class="c1"># Batch size</span>
            <span class="s2">&quot;LR&quot;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span>  <span class="c1"># Learning rate</span>
            <span class="s2">&quot;GAMMA&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>  <span class="c1"># Discount factor</span>
            <span class="s2">&quot;MEMORY_SIZE&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>  <span class="c1"># Max memory buffer size</span>
            <span class="s2">&quot;LEARN_STEP&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># Learning frequency</span>
            <span class="s2">&quot;N_STEP&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># Step number to calculate td error</span>
            <span class="s2">&quot;PER&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Use prioritized experience replay buffer</span>
            <span class="s2">&quot;ALPHA&quot;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>  <span class="c1"># Prioritized replay buffer parameter</span>
            <span class="s2">&quot;TAU&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>  <span class="c1"># For soft update of target parameters</span>
            <span class="s2">&quot;BETA&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>  <span class="c1"># Importance sampling coefficient</span>
            <span class="s2">&quot;PRIOR_EPS&quot;</span><span class="p">:</span> <span class="mf">0.000001</span><span class="p">,</span>  <span class="c1"># Minimum priority for sampling</span>
            <span class="s2">&quot;NUM_ATOMS&quot;</span><span class="p">:</span> <span class="mi">51</span><span class="p">,</span>  <span class="c1"># Unit number of support</span>
            <span class="s2">&quot;V_MIN&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>  <span class="c1"># Minimum value of support</span>
            <span class="s2">&quot;V_MAX&quot;</span><span class="p">:</span> <span class="mf">200.0</span><span class="p">,</span>  <span class="c1"># Maximum value of support</span>
            <span class="s2">&quot;WANDB&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Use Weights and Biases tracking</span>
        <span class="p">}</span>

        <span class="c1"># Define the connect four environment</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">connect_four_v3</span><span class="o">.</span><span class="n">env</span><span class="p">()</span>
        <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="c1"># Configure the algo input arguments</span>
        <span class="n">state_dim</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)[</span><span class="s2">&quot;observation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span>
        <span class="p">]</span>
        <span class="n">one_hot</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">action_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span><span class="o">.</span><span class="n">n</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">]</span>
        <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;DISCRETE_ACTIONS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;MAX_ACTION&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;MIN_ACTION&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Warp the environment in the curriculum learning wrapper</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">CurriculumEnv</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">LESSON</span><span class="p">)</span>

        <span class="c1"># Pre-process dimensions for PyTorch layers</span>
        <span class="c1"># We only need to worry about the state dim of a single agent</span>
        <span class="c1"># We flatten the 6x7x2 observation as input to the agent&quot;s neural network</span>
        <span class="n">state_dim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Create a population ready for evolutionary hyper-parameter optimisation</span>
        <span class="n">pop</span> <span class="o">=</span> <span class="n">initialPopulation</span><span class="p">(</span>
            <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;ALGO&quot;</span><span class="p">],</span>
            <span class="n">state_dim</span><span class="p">,</span>
            <span class="n">action_dim</span><span class="p">,</span>
            <span class="n">one_hot</span><span class="p">,</span>
            <span class="n">NET_CONFIG</span><span class="p">,</span>
            <span class="n">INIT_HP</span><span class="p">,</span>
            <span class="n">population_size</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;POPULATION_SIZE&quot;</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Configure the replay buffer</span>
        <span class="n">field_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;next_state&quot;</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">]</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span>
            <span class="n">action_dim</span><span class="o">=</span><span class="n">action_dim</span><span class="p">,</span>  <span class="c1"># Number of agent actions</span>
            <span class="n">memory_size</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;MEMORY_SIZE&quot;</span><span class="p">],</span>  <span class="c1"># Max replay buffer size</span>
            <span class="n">field_names</span><span class="o">=</span><span class="n">field_names</span><span class="p">,</span>  <span class="c1"># Field names to store in memory</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Instantiate a tournament selection object (used for HPO)</span>
        <span class="n">tournament</span> <span class="o">=</span> <span class="n">TournamentSelection</span><span class="p">(</span>
            <span class="n">tournament_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Tournament selection size</span>
            <span class="n">elitism</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Elitism in tournament selection</span>
            <span class="n">population_size</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;POPULATION_SIZE&quot;</span><span class="p">],</span>  <span class="c1"># Population size</span>
            <span class="n">evo_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># Evaluate using last N fitness scores</span>

        <span class="c1"># Instantiate a mutations object (used for HPO)</span>
        <span class="n">mutations</span> <span class="o">=</span> <span class="n">Mutations</span><span class="p">(</span>
            <span class="n">algo</span><span class="o">=</span><span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;ALGO&quot;</span><span class="p">],</span>
            <span class="n">no_mutation</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of no mutation</span>
            <span class="n">architecture</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Probability of architecture mutation</span>
            <span class="n">new_layer_prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of new layer mutation</span>
            <span class="n">parameters</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of parameter mutation</span>
            <span class="n">activation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Probability of activation function mutation</span>
            <span class="n">rl_hp</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Probability of RL hyperparameter mutation</span>
            <span class="n">rl_hp_selection</span><span class="o">=</span><span class="p">[</span>
                <span class="s2">&quot;lr&quot;</span><span class="p">,</span>
                <span class="s2">&quot;learn_step&quot;</span><span class="p">,</span>
                <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
            <span class="p">],</span>  <span class="c1"># RL hyperparams selected for mutation</span>
            <span class="n">mutation_sd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># Mutation strength</span>
            <span class="c1"># Define search space for each hyperparameter</span>
            <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
            <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
            <span class="n">min_learn_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">max_learn_step</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
            <span class="n">min_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">arch</span><span class="o">=</span><span class="n">NET_CONFIG</span><span class="p">[</span><span class="s2">&quot;arch&quot;</span><span class="p">],</span>  <span class="c1"># MLP or CNN</span>
            <span class="n">rand_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Define training loop parameters</span>
        <span class="n">episodes_per_epoch</span> <span class="o">=</span> <span class="mi">10</span>

        <span class="c1"># ! NOTE: Uncomment the max_episodes line below to change the number of training episodes. ! #</span>
        <span class="c1"># It is deliberately set low to allow testing to ensure this tutorial is sound.</span>
        <span class="n">max_episodes</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="c1"># max_episodes = LESSON[&quot;max_train_episodes&quot;]  # Total episodes</span>

        <span class="n">max_steps</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Maximum steps to take in each episode</span>
        <span class="n">evo_epochs</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Evolution frequency</span>
        <span class="n">evo_loop</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># Number of evaluation episodes</span>
        <span class="n">elite</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Assign a placeholder &quot;elite&quot; agent</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Starting epsilon value</span>
        <span class="n">eps_end</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Final epsilon value</span>
        <span class="n">eps_decay</span> <span class="o">=</span> <span class="mf">0.9998</span>  <span class="c1"># Epsilon decays</span>
        <span class="n">opp_update_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">wb</span> <span class="o">=</span> <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;WANDB&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;pretrained_path&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
                <span class="c1"># Load pretrained checkpoint</span>
                <span class="n">agent</span><span class="o">.</span><span class="n">loadCheckpoint</span><span class="p">(</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;pretrained_path&quot;</span><span class="p">])</span>
                <span class="c1"># Reinit optimizer for new task</span>
                <span class="n">agent</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;LR&quot;</span><span class="p">]</span>
                <span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
                    <span class="n">agent</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">agent</span><span class="o">.</span><span class="n">lr</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
            <span class="c1"># Create initial pool of opponents</span>
            <span class="n">opponent_pool</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent_pool_size&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent_pool_size&quot;</span><span class="p">]):</span>
                <span class="n">opp</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">opp</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
                <span class="n">opp</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="n">opponent_pool</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">opp</span><span class="p">)</span>

        <span class="c1"># Perform buffer and agent warmups if desired</span>
        <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;buffer_warm_up&quot;</span><span class="p">]:</span>
            <span class="n">warm_up_opponent</span> <span class="o">=</span> <span class="n">Opponent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;warm_up_opponent&quot;</span><span class="p">])</span>
            <span class="n">memory</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fill_replay_buffer</span><span class="p">(</span>
                <span class="n">memory</span><span class="p">,</span> <span class="n">warm_up_opponent</span>
            <span class="p">)</span>  <span class="c1"># Fill replay buffer with transitions</span>
            <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;agent_warm_up&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warming up agents ...&quot;</span><span class="p">)</span>
                <span class="n">agent</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1"># Train on randomly collected samples</span>
                <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;agent_warm_up&quot;</span><span class="p">]):</span>
                    <span class="n">experiences</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
                    <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">experiences</span><span class="p">)</span>
                <span class="n">pop</span> <span class="o">=</span> <span class="p">[</span><span class="n">agent</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">]</span>
                <span class="n">elite</span> <span class="o">=</span> <span class="n">agent</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Agent population warmed up.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">max_episodes</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">wb</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
                    <span class="c1"># set the wandb project where this run will be logged</span>
                    <span class="n">project</span><span class="o">=</span><span class="s2">&quot;AgileRL&quot;</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">-EvoHPO-</span><span class="si">{}</span><span class="s2">-</span><span class="si">{}</span><span class="s2">Opposition-CNN-</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="s2">&quot;connect_four_v3&quot;</span><span class="p">,</span>
                        <span class="n">INIT_HP</span><span class="p">[</span><span class="s2">&quot;ALGO&quot;</span><span class="p">],</span>
                        <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">],</span>
                        <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%m</span><span class="si">%d</span><span class="s2">%Y%H%M%S&quot;</span><span class="p">),</span>
                    <span class="p">),</span>
                    <span class="c1"># track hyperparameters and run metadata</span>
                    <span class="n">config</span><span class="o">=</span><span class="p">{</span>
                        <span class="s2">&quot;algo&quot;</span><span class="p">:</span> <span class="s2">&quot;Evo HPO Rainbow DQN&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="s2">&quot;connect_four_v3&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;INIT_HP&quot;</span><span class="p">:</span> <span class="n">INIT_HP</span><span class="p">,</span>
                        <span class="s2">&quot;lesson&quot;</span><span class="p">:</span> <span class="n">LESSON</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">)</span>

        <span class="n">total_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total_episodes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">pbar</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">max_episodes</span> <span class="o">/</span> <span class="n">episodes_per_epoch</span><span class="p">))</span>

        <span class="c1"># Training loop</span>
        <span class="k">for</span> <span class="n">idx_epi</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="n">turns_per_episode</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">train_actions_hist</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">action_dim</span>
            <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>  <span class="c1"># Loop through population</span>
                <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes_per_epoch</span><span class="p">):</span>
                    <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># Reset environment at start of episode</span>
                    <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

                    <span class="p">(</span>
                        <span class="n">p1_state</span><span class="p">,</span>
                        <span class="n">p1_state_flipped</span><span class="p">,</span>
                        <span class="n">p1_action</span><span class="p">,</span>
                        <span class="n">p1_next_state</span><span class="p">,</span>
                        <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                        <span class="c1"># Randomly choose opponent from opponent pool if using self-play</span>
                        <span class="n">opponent</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">opponent_pool</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Create opponent of desired difficulty</span>
                        <span class="n">opponent</span> <span class="o">=</span> <span class="n">Opponent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">])</span>

                    <span class="c1"># Randomly decide whether agent will go first or second</span>
                    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                        <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">True</span>

                    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">turns</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Number of turns counter</span>

                    <span class="k">for</span> <span class="n">idx_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
                        <span class="c1"># Player 0&quot;s turn</span>
                        <span class="n">p0_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                        <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
                        <span class="n">p0_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                        <span class="n">p0_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                        <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                                <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                    <span class="n">p0_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p0_action_mask</span>
                                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                            <span class="k">elif</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                                <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                    <span class="n">p0_action_mask</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">]</span>
                                <span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">p0_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">p0_action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                <span class="n">p0_state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">p0_action_mask</span>
                            <span class="p">)[</span>
                                <span class="mi">0</span>
                            <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                            <span class="n">train_actions_hist</span><span class="p">[</span><span class="n">p0_action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                        <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p0_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                        <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
                        <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                            <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                        <span class="p">)</span>
                        <span class="n">p0_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span>
                        <span class="p">)</span>
                        <span class="n">p0_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                        <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                            <span class="n">score</span> <span class="o">+=</span> <span class="n">env_reward</span>
                        <span class="n">turns</span> <span class="o">+=</span> <span class="mi">1</span>

                        <span class="c1"># Check if game is over (Player 0 win)</span>
                        <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                            <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                            <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                    <span class="p">(</span>
                                        <span class="n">p0_state</span><span class="p">,</span>
                                        <span class="n">p1_state</span><span class="p">,</span>
                                        <span class="n">p0_state_flipped</span><span class="p">,</span>
                                        <span class="n">p1_state_flipped</span><span class="p">,</span>
                                    <span class="p">)</span>
                                <span class="p">),</span>
                                <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                                <span class="p">[</span>
                                    <span class="n">reward</span><span class="p">,</span>
                                    <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                    <span class="n">reward</span><span class="p">,</span>
                                    <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                <span class="p">],</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                    <span class="p">(</span>
                                        <span class="n">p0_next_state</span><span class="p">,</span>
                                        <span class="n">p1_next_state</span><span class="p">,</span>
                                        <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                                        <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                                    <span class="p">)</span>
                                <span class="p">),</span>
                                <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                            <span class="k">if</span> <span class="n">p1_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                                <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p1_state</span><span class="p">,</span> <span class="n">p1_state_flipped</span><span class="p">)),</span>
                                    <span class="p">[</span><span class="n">p1_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">],</span>
                                    <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                        <span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="n">p1_next_state_flipped</span><span class="p">)</span>
                                    <span class="p">),</span>
                                    <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                                <span class="p">)</span>

                            <span class="c1"># Player 1&quot;s turn</span>
                            <span class="n">p1_action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                            <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                                <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                            <span class="p">)</span>
                            <span class="c1"># Swap pieces so that the agent always sees the board from the same perspective</span>
                            <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                            <span class="n">p1_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                            <span class="n">p1_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                            <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                                    <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                        <span class="n">p1_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p1_action_mask</span>
                                    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                                <span class="k">elif</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                                    <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                        <span class="n">p1_action_mask</span><span class="p">,</span>
                                        <span class="n">p0_action</span><span class="p">,</span>
                                        <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;block_vert_coef&quot;</span><span class="p">],</span>
                                    <span class="p">)</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">p1_action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">p1_action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span>
                                    <span class="n">p1_state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">p1_action_mask</span>
                                <span class="p">)[</span>
                                    <span class="mi">0</span>
                                <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                                <span class="n">train_actions_hist</span><span class="p">[</span><span class="n">p1_action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                            <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">p1_action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                            <span class="n">observation</span><span class="p">,</span> <span class="n">env_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
                            <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                                <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                            <span class="p">)</span>
                            <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p1_next_state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                            <span class="n">p1_next_state_flipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span>
                            <span class="p">)</span>
                            <span class="n">p1_next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p1_next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                            <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                                <span class="n">score</span> <span class="o">+=</span> <span class="n">env_reward</span>
                            <span class="n">turns</span> <span class="o">+=</span> <span class="mi">1</span>

                            <span class="c1"># Check if game is over (Player 1 win)</span>
                            <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                                <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                                <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                        <span class="p">(</span>
                                            <span class="n">p0_state</span><span class="p">,</span>
                                            <span class="n">p1_state</span><span class="p">,</span>
                                            <span class="n">p0_state_flipped</span><span class="p">,</span>
                                            <span class="n">p1_state_flipped</span><span class="p">,</span>
                                        <span class="p">)</span>
                                    <span class="p">),</span>
                                    <span class="p">[</span>
                                        <span class="n">p0_action</span><span class="p">,</span>
                                        <span class="n">p1_action</span><span class="p">,</span>
                                        <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">,</span>
                                        <span class="mi">6</span> <span class="o">-</span> <span class="n">p1_action</span><span class="p">,</span>
                                    <span class="p">],</span>
                                    <span class="p">[</span>
                                        <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                        <span class="n">reward</span><span class="p">,</span>
                                        <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">][</span><span class="s2">&quot;lose&quot;</span><span class="p">],</span>
                                        <span class="n">reward</span><span class="p">,</span>
                                    <span class="p">],</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                        <span class="p">(</span>
                                            <span class="n">p0_next_state</span><span class="p">,</span>
                                            <span class="n">p1_next_state</span><span class="p">,</span>
                                            <span class="n">p0_next_state_flipped</span><span class="p">,</span>
                                            <span class="n">p1_next_state_flipped</span><span class="p">,</span>
                                        <span class="p">)</span>
                                    <span class="p">),</span>
                                    <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                                <span class="p">)</span>

                            <span class="k">else</span><span class="p">:</span>  <span class="c1"># Play continues</span>
                                <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                                <span class="n">memory</span><span class="o">.</span><span class="n">save2memoryVectEnvs</span><span class="p">(</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">p0_state</span><span class="p">,</span> <span class="n">p0_state_flipped</span><span class="p">)),</span>
                                    <span class="p">[</span><span class="n">p0_action</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="n">p0_action</span><span class="p">],</span>
                                    <span class="p">[</span><span class="n">reward</span><span class="p">,</span> <span class="n">reward</span><span class="p">],</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                                        <span class="p">(</span><span class="n">p0_next_state</span><span class="p">,</span> <span class="n">p0_next_state_flipped</span><span class="p">)</span>
                                    <span class="p">),</span>
                                    <span class="p">[</span><span class="n">done</span><span class="p">,</span> <span class="n">done</span><span class="p">],</span>
                                <span class="p">)</span>

                        <span class="c1"># Learn according to learning frequency</span>
                        <span class="k">if</span> <span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">counter</span> <span class="o">%</span> <span class="n">agent</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                            <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">agent</span><span class="o">.</span><span class="n">batch_size</span>
                        <span class="p">):</span>
                            <span class="c1"># Sample replay buffer</span>
                            <span class="c1"># Learn according to agent&quot;s RL algorithm</span>
                            <span class="n">experiences</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
                            <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">experiences</span><span class="p">)</span>

                        <span class="c1"># Stop episode if any agents have terminated</span>
                        <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                            <span class="k">break</span>

                    <span class="n">total_steps</span> <span class="o">+=</span> <span class="n">idx_step</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">total_episodes</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">turns_per_episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">turns</span><span class="p">)</span>
                    <span class="c1"># Save the total episode reward</span>
                    <span class="n">agent</span><span class="o">.</span><span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
                        <span class="k">if</span> <span class="p">(</span><span class="n">total_episodes</span> <span class="o">%</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;opponent_upgrade&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                            <span class="p">(</span><span class="n">idx_epi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">evo_epochs</span>
                        <span class="p">):</span>
                            <span class="n">elite_opp</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tournament</span><span class="o">.</span><span class="n">_elitism</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
                            <span class="n">elite_opp</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                            <span class="n">opponent_pool</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elite_opp</span><span class="p">)</span>
                            <span class="n">opp_update_counter</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Update epsilon for exploration</span>
                <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">eps_end</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">eps_decay</span><span class="p">)</span>

            <span class="n">mean_turns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">turns_per_episode</span><span class="p">)</span>

            <span class="c1"># Now evolve population if necessary</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">idx_epi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">evo_epochs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Evaluate population vs random actions</span>
                <span class="n">fitnesses</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">win_rates</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">eval_actions_hist</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">action_dim</span>  <span class="c1"># Eval actions histogram</span>
                <span class="n">eval_turns</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Eval turns counter</span>
                <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evo_loop</span><span class="p">):</span>
                            <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># Reset environment at start of episode</span>
                            <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

                            <span class="n">player</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Tracker for which player&quot;s turn it is</span>

                            <span class="c1"># Create opponent of desired difficulty</span>
                            <span class="n">opponent</span> <span class="o">=</span> <span class="n">Opponent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">difficulty</span><span class="o">=</span><span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;eval_opponent&quot;</span><span class="p">])</span>

                            <span class="c1"># Randomly decide whether agent will go first or second</span>
                            <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                                <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">False</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">opponent_first</span> <span class="o">=</span> <span class="kc">True</span>

                            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

                            <span class="k">for</span> <span class="n">idx_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
                                <span class="n">action_mask</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;action_mask&quot;</span><span class="p">]</span>
                                <span class="k">if</span> <span class="n">player</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                                    <span class="k">if</span> <span class="n">opponent_first</span><span class="p">:</span>
                                        <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;eval_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">action_mask</span><span class="p">)</span>
                                        <span class="k">else</span><span class="p">:</span>
                                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                                            <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                                        <span class="p">)</span>
                                        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                                        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">)[</span>
                                            <span class="mi">0</span>
                                        <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                                        <span class="n">eval_actions_hist</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                                <span class="k">if</span> <span class="n">player</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                                    <span class="k">if</span> <span class="ow">not</span> <span class="n">opponent_first</span><span class="p">:</span>
                                        <span class="k">if</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;eval_opponent&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">action_mask</span><span class="p">)</span>
                                        <span class="k">else</span><span class="p">:</span>
                                            <span class="n">action</span> <span class="o">=</span> <span class="n">opponent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">player</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span>
                                            <span class="n">observation</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                                        <span class="p">)</span>
                                        <span class="n">state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span>
                                        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                                        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">getAction</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">)[</span>
                                            <span class="mi">0</span>
                                        <span class="p">]</span>  <span class="c1"># Get next action from agent</span>
                                        <span class="n">eval_actions_hist</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                                <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># Act in environment</span>
                                <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>

                                <span class="k">if</span> <span class="p">(</span><span class="n">player</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">opponent_first</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                                    <span class="n">player</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">opponent_first</span>
                                <span class="p">):</span>
                                    <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>

                                <span class="n">eval_turns</span> <span class="o">+=</span> <span class="mi">1</span>

                                <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncation</span><span class="p">:</span>
                                    <span class="k">break</span>

                                <span class="n">player</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

                            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
                    <span class="n">mean_fit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
                    <span class="n">agent</span><span class="o">.</span><span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_fit</span><span class="p">)</span>
                    <span class="n">fitnesses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_fit</span><span class="p">)</span>

                <span class="n">eval_turns</span> <span class="o">=</span> <span class="n">eval_turns</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span> <span class="o">/</span> <span class="n">evo_loop</span>

                <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix_str</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;    Train Mean Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="n">episodes_per_epoch</span><span class="p">:])</span><span class="si">}</span><span class="s2">   Train Mean Turns: </span><span class="si">{</span><span class="n">mean_turns</span><span class="si">}</span><span class="s2">   Eval Mean Fitness: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">)</span><span class="si">}</span><span class="s2">   Eval Best Fitness: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">)</span><span class="si">}</span><span class="s2">   Eval Mean Turns: </span><span class="si">{</span><span class="n">eval_turns</span><span class="si">}</span><span class="s2">   Total Steps: </span><span class="si">{</span><span class="n">total_steps</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Format action histograms for visualisation</span>
                <span class="n">train_actions_hist</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">freq</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">train_actions_hist</span><span class="p">)</span> <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">train_actions_hist</span>
                <span class="p">]</span>
                <span class="n">eval_actions_hist</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">freq</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">eval_actions_hist</span><span class="p">)</span> <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">eval_actions_hist</span>
                <span class="p">]</span>
                <span class="n">train_actions_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="sa">f</span><span class="s2">&quot;train/action_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">action</span>
                    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_actions_hist</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="n">eval_actions_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="sa">f</span><span class="s2">&quot;eval/action_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">action</span>
                    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">eval_actions_hist</span><span class="p">)</span>
                <span class="p">}</span>

                <span class="k">if</span> <span class="n">wb</span><span class="p">:</span>
                    <span class="n">wandb_dict</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">&quot;global_step&quot;</span><span class="p">:</span> <span class="n">total_steps</span><span class="p">,</span>
                        <span class="s2">&quot;train/mean_score&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="n">episodes_per_epoch</span><span class="p">:]),</span>
                        <span class="s2">&quot;train/mean_turns_per_game&quot;</span><span class="p">:</span> <span class="n">mean_turns</span><span class="p">,</span>
                        <span class="s2">&quot;train/epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span>
                        <span class="s2">&quot;train/opponent_updates&quot;</span><span class="p">:</span> <span class="n">opp_update_counter</span><span class="p">,</span>
                        <span class="s2">&quot;eval/mean_fitness&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">),</span>
                        <span class="s2">&quot;eval/best_fitness&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fitnesses</span><span class="p">),</span>
                        <span class="s2">&quot;eval/mean_turns_per_game&quot;</span><span class="p">:</span> <span class="n">eval_turns</span><span class="p">,</span>
                    <span class="p">}</span>
                    <span class="n">wandb_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">train_actions_dict</span><span class="p">)</span>
                    <span class="n">wandb_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">eval_actions_dict</span><span class="p">)</span>
                    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">wandb_dict</span><span class="p">)</span>

                <span class="c1"># Tournament selection and population mutation</span>
                <span class="n">elite</span><span class="p">,</span> <span class="n">pop</span> <span class="o">=</span> <span class="n">tournament</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
                <span class="n">pop</span> <span class="o">=</span> <span class="n">mutations</span><span class="o">.</span><span class="n">mutation</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">max_episodes</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">wb</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>

        <span class="c1"># Save the trained agent</span>
        <span class="n">save_path</span> <span class="o">=</span> <span class="n">LESSON</span><span class="p">[</span><span class="s2">&quot;save_path&quot;</span><span class="p">]</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">save_path</span><span class="p">),</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">elite</span><span class="o">.</span><span class="n">saveCheckpoint</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Elite agent saved to &#39;</span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
</section>
</section>

          </article>
        </div>
        <footer>
          
          <div class="related-pages">
            <a class="next-page" href="../MADDPG/">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">AgileRL: Implementing MADDPG</div>
              </div>
              <svg class="furo-related-icon">
                <use href="#svg-arrow-right"></use>
              </svg>
            </a>
            <a class="prev-page" href="../">
              <svg class="furo-related-icon">
                <use href="#svg-arrow-right"></use>
              </svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">AgileRL Tutorial</div>
                
              </div>
            </a>
          </div>
          <div class="bottom-of-page">
            <div class="left-details">
              <div class="copyright">
                Copyright &#169; 2023 Farama Foundation
              </div>
              <!--
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            -->
            </div>
            <div class="right-details">
              <div class="icons">
                <a class="muted-link" href="https://github.com/Farama-Foundation/PettingZoo/"
                  aria-label="On GitHub">
                  <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd"
                      d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z">
                    </path>
                  </svg>
                </a>
              </div>
            </div>
          </div>
          
        </footer>
      </div>
      <aside class="toc-drawer">
        
        
        <div class="toc-sticky toc-scroll">
          <div class="toc-title-container">
            <span class="toc-title">
              On this page
            </span>
          </div>
          <div class="toc-tree-container">
            <div class="toc-tree">
              <ul>
<li><a class="reference internal" href="#">AgileRL: Implementing DQN - Curriculum Learning and Self-play</a><ul>
<li><a class="reference internal" href="#what-is-dqn">What is DQN?</a><ul>
<li><a class="reference internal" href="#can-i-use-it">Can I use it?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#environment-setup">Environment Setup</a></li>
<li><a class="reference internal" href="#code">Code</a><ul>
<li><a class="reference internal" href="#curriculum-learning-and-self-play-using-dqn-on-connect-four">Curriculum learning and self-play using DQN on Connect Four</a></li>
<li><a class="reference internal" href="#imports">Imports</a></li>
<li><a class="reference internal" href="#curriculum-learning">Curriculum Learning</a><ul>
<li><a class="reference internal" href="#config-files">Config files</a></li>
</ul>
</li>
<li><a class="reference internal" href="#general-setup">General setup</a></li>
<li><a class="reference internal" href="#self-play">Self-play</a></li>
<li><a class="reference internal" href="#training-loop">Training loop</a></li>
<li><a class="reference internal" href="#trained-model-weights">Trained model weights</a></li>
<li><a class="reference internal" href="#watch-the-trained-agents-play">Watch the trained agents play</a></li>
<li><a class="reference internal" href="#full-training-code">Full training code</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
        
        
      </aside>
    </div>
  </div>
</div>
    <script>
      const toggleMenu = () => {
        const menuBtn = document.querySelector(".farama-header-menu__btn");
        const menuContainer = document.querySelector(".farama-header-menu-container");
        if (document.querySelector(".farama-header-menu").classList.contains("active")) {
          menuBtn.setAttribute("aria-expanded", "false");
          menuContainer.setAttribute("aria-hidden", "true");
        } else {
          menuBtn.setAttribute("aria-expanded", "true");
          menuContainer.setAttribute("aria-hidden", "false");
        }
        document.querySelector(".farama-header-menu").classList.toggle("active");
      }

      document.querySelector(".farama-header-menu__btn").addEventListener("click", toggleMenu);
      document.getElementById("farama-close-menu").addEventListener("click", toggleMenu);
    </script>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q4EGMJ3R24"></script>
      <script>
        const enableGtag = () => {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-Q4EGMJ3R24');
        }
        (() => {
            if (!localStorage.getItem("acceptedCookieAlert")) {
                const boxElem = document.createElement("div");
                boxElem.classList.add("cookie-alert");
                const containerElem = document.createElement("div");
                containerElem.classList.add("cookie-alert__container");
                const textElem = document.createElement("p");
                textElem.innerHTML = `This page uses <a href="https://analytics.google.com/">
                                    Google Analytics</a> to collect statistics.`;
                                    containerElem.appendChild(textElem);

                const declineBtn = Object.assign(document.createElement("button"),
                  {
                    innerText: "Deny",
                    className: "farama-btn cookie-alert__button",
                    id: "cookie-alert__decline",
                  }
                );
                declineBtn.addEventListener("click", () => {
                  localStorage.setItem("acceptedCookieAlert", false);
                  boxElem.remove();
                });

                const acceptBtn = Object.assign(document.createElement("button"),
                  {
                    innerText: "Allow",
                    className: "farama-btn cookie-alert__button",
                    id: "cookie-alert__accept",
                  }
                );
                acceptBtn.addEventListener("click", () => {
                  localStorage.setItem("acceptedCookieAlert", true);
                  boxElem.remove();
                  enableGtag();
                });

                containerElem.appendChild(declineBtn);
                containerElem.appendChild(acceptBtn);
                boxElem.appendChild(containerElem);
                document.body.appendChild(boxElem);
            } else if (localStorage.getItem("acceptedCookieAlert") === "true") {
              enableGtag();
            }
        })()
      </script>

    <script src="../../../_static/documentation_options.js?v=4b4b37dc"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/furo.js?v=7660844c"></script>
    
    <script>

      const createProjectsList = (projects, displayImages) => {
        const ulElem = Object.assign(document.createElement('ul'),
          {
            className:'farama-header-menu-list',
          }
        )
        for (let project of projects) {
          const liElem = document.createElement("li");
          const aElem = Object.assign(document.createElement("a"),
            {
              href: project.link
            }
          );
          liElem.appendChild(aElem);
          if (displayImages) {
            const imgElem = Object.assign(document.createElement("img"),
              {
                src: project.image ? imagesBasepath + project.image : imagesBasepath + "/farama_black.svg",
                alt: `${project.name} logo`,
                className: "farama-black-logo-invert"
              }
            );
            aElem.appendChild(imgElem);
          }
          aElem.appendChild(document.createTextNode(project.name));
          ulElem.appendChild(liElem);
        }
        return ulElem;
      }

      // Create menu with Farama projects by using the API at farama.org/api/projects.json
      const createCORSRequest = (method, url) => {
        let xhr = new XMLHttpRequest();
        xhr.responseType = 'json';

        if ("withCredentials" in xhr) {
          xhr.open(method, url, true);
        } else if (typeof XDomainRequest != "undefined") {
          // IE8 & IE9
          xhr = new XDomainRequest();
          xhr.open(method, url);
        } else {
          // CORS not supported.
          xhr = null;
        }
        return xhr;
      };

      const url = 'https://farama.org/api/projects.json';
      const imagesBasepath = "https://farama.org/assets/images"
      const method = 'GET';
      let xhr = createCORSRequest(method, url);

      xhr.onload = () => {
        const jsonResponse = xhr.response;
        const sections = {
          "Core Projects": [],
          "Mature Projects": {
            "Documentation": [],
            "Repositories": [],
          },
          "Incubating Projects": {
            "Documentation": [],
            "Repositories": [],
          },
          "Foundation": [
            {
              name: "About",
              link: "https://farama.org/about"
            },
            {
              name: "Standards",
              link: "https://farama.org/project_standards",
            },
            {
              name: "Donate",
              link: "https://farama.org/donations"
            }
          ]
        }

        // Categorize projects
        Object.keys(jsonResponse).forEach(key => {
          projectJson = jsonResponse[key];
          if (projectJson.website !== null) {
            projectJson.link = projectJson.website;
          } else {
            projectJson.link = projectJson.github;
          }
          if (projectJson.type === "core") {
            sections["Core Projects"].push(projectJson)
          } else if (projectJson.type == "mature") {
            if (projectJson.website !== null) {
              sections["Mature Projects"]["Documentation"].push(projectJson)
            } else {
              sections["Mature Projects"]["Repositories"].push(projectJson)
            }
          } else {
            if (projectJson.website !== null) {
              sections["Incubating Projects"]["Documentation"].push(projectJson)
            } else {
              sections["Incubating Projects"]["Repositories"].push(projectJson)
            }
          }
        })

        const menuContainer = document.querySelector(".farama-header-menu__body");

        Object.keys(sections).forEach((key, i) => {
          const sectionElem = Object.assign(
            document.createElement('div'), {
              className:'farama-header-menu__section',
            }
          )
          sectionElem.appendChild(Object.assign(document.createElement('span'),
            {
              className:'farama-header-menu__section-title' ,
              innerText: key
            }
          ))
          // is not a list
          if (sections[key].constructor !== Array) {
            const subSections = sections[key];
            const subSectionContainerElem = Object.assign(
                document.createElement('div'), {
                  className:'farama-header-menu__subsections-container',
                  style: 'display: flex'
                }
            )
            Object.keys(subSections).forEach((subKey, i) => {
              const subSectionElem = Object.assign(
                document.createElement('div'), {
                  className:'farama-header-menu__subsection',
                }
              )
              subSectionElem.appendChild(Object.assign(document.createElement('span'),
                {
                  className:'farama-header-menu__subsection-title' ,
                  innerText: subKey
                }
              ))
              const ulElem = createProjectsList(subSections[subKey], key !== 'Foundation');
              subSectionElem.appendChild(ulElem);
              subSectionContainerElem.appendChild(subSectionElem);
            })
            sectionElem.appendChild(subSectionContainerElem);
          } else {
            const projects = sections[key];
            const ulElem = createProjectsList(projects, true);
            sectionElem.appendChild(ulElem);
          }
          menuContainer.appendChild(sectionElem)
        });
      }

      xhr.onerror = function() {
        console.error("Unable to load projects");
      };

      xhr.send();
    </script>

    
    <script>
      const versioningConfig = {
        githubUser: 'Farama-Foundation',
        githubRepo: 'PettingZoo',
      };
      fetch('/main/_static/versioning/versioning_menu.html').then(response => {
        if (response.status === 200) {
            response.text().then(text => {
                const container = document.createElement("div");
                container.innerHTML = text;
                document.querySelector("body").appendChild(container);
                // innerHtml doenst evaluate scripts, we need to add them dynamically
                Array.from(container.querySelectorAll("script")).forEach(oldScript => {
                    const newScript = document.createElement("script");
                    Array.from(oldScript.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value));
                    newScript.appendChild(document.createTextNode(oldScript.innerHTML));
                    oldScript.parentNode.replaceChild(newScript, oldScript);
                });
            });
        } else {
            console.warn("Unable to load versioning menu", response);
        }
      });
    </script>

    </body>
</html>